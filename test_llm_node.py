#!/usr/bin/env python3
"""
æµ‹è¯•LLMè½¬å‘èŠ‚ç‚¹
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.abspath(os.path.dirname(__file__))
if project_root not in sys.path:    
    sys.path.insert(0, project_root)

async def test_llm_node():
    """æµ‹è¯•LLMè½¬å‘èŠ‚ç‚¹"""
    print("ğŸ”§ æµ‹è¯•LLMè½¬å‘èŠ‚ç‚¹é…ç½®...")
    
    try:
        # å¯¼å…¥å¿…è¦æ¨¡å—
        from src.workflow.graph.scenario_workflow import llm_forwarding_node
        from config.manager import settings
        
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   - proxy.target_url: {settings.proxy.target_url}")
        print(f"   - agent.api_key: {settings.agent.api_key[:8]}...{settings.agent.api_key[-8:]}")
        
        # åˆ›å»ºæµ‹è¯•çŠ¶æ€
        test_state = {
            "original_messages": [{"role": "user", "content": "ä½ å¥½ï¼Œæµ‹è¯•æ¶ˆæ¯"}],
            "messages": [{"role": "user", "content": "ä½ å¥½ï¼Œæµ‹è¯•æ¶ˆæ¯"}], 
            "api_key": "sk-5b155b212651493b942e7dca7dfb4751",
            "model": "deepseek-chat",
            "stream": True
        }
        
        print("ğŸš€ å¼€å§‹æµ‹è¯•LLMè½¬å‘èŠ‚ç‚¹...")
        
        # æµ‹è¯•èŠ‚ç‚¹
        result = await llm_forwarding_node(test_state)
        
        # æ£€æŸ¥ç»“æœ
        if "llm_response" in result:
            llm_response = result["llm_response"]
            if hasattr(llm_response, 'content'):
                content = llm_response.content
                print(f"âœ… LLMèŠ‚ç‚¹æˆåŠŸè¿”å›å†…å®¹: {content[:100]}...")
                return True
            elif "Error" in str(llm_response):
                print(f"âŒ LLMèŠ‚ç‚¹è¿”å›é”™è¯¯: {str(llm_response)}")
                return False
            else:
                print(f"âœ… LLMèŠ‚ç‚¹è¿”å›å“åº”: {str(llm_response)[:100]}...")
                return True
        else:
            print("âŒ LLMèŠ‚ç‚¹æœªè¿”å›å“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = asyncio.run(test_llm_node())
    if success:
        print("\nğŸ‰ LLMè½¬å‘èŠ‚ç‚¹æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nğŸ’¥ LLMè½¬å‘èŠ‚ç‚¹æµ‹è¯•å¤±è´¥ï¼")