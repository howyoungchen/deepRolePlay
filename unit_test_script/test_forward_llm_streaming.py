#!/usr/bin/env python3
"""
æµ‹è¯•forward_to_llm_streamingå‡½æ•°
æ¨¡æ‹Ÿå‰ç«¯è°ƒç”¨ï¼Œä½¿ç”¨DeepSeeké…ç½®ï¼Œæ‰“å°å®Œæ•´çš„æ¨ç†+æ­£æ–‡å†…å®¹
"""
import asyncio
import sys
import os
import time

# Add project root to Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

async def test_forward_llm_streaming():
    """æµ‹è¯•forward_to_llm_streamingå‡½æ•°å¹¶æ‰“å°å®Œæ•´å†…å®¹"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•forward_to_llm_streamingå‡½æ•°...")
    
    try:
        # å¯¼å…¥æ‰€éœ€æ¨¡å—
        from src.workflow.graph.scenario_workflow import forward_to_llm_streaming
        
        # æ¨¡æ‹Ÿå‰ç«¯é…ç½® - ä½¿ç”¨ç”¨æˆ·æä¾›çš„DeepSeekå‚æ•°
        api_key = "sk-5b155b212651493b942e7dca7dfb4751"
        model = "deepseek-reasoner"
        
        # å‡†å¤‡æµ‹è¯•æ¶ˆæ¯
        original_messages = [
            {"role": "user", "content": "æ‰®æ¼”çŒ«å’ªï¼Œ100å­—ä»¥å†…"}
        ]
        
        print(f"ğŸ“¤ æµ‹è¯•æ¶ˆæ¯: {original_messages[0]['content']}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
        print(f"ğŸŒ APIåœ°å€: https://api.deepseek.com/v1")
        print("\n" + "="*50)
        print("ğŸ“¥ å¼€å§‹æ¥æ”¶æµå¼å“åº”...")
        print("="*50)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # ç”¨äºæ”¶é›†å®Œæ•´å†…å®¹
        full_content = ""
        chunk_count = 0
        
        # è°ƒç”¨forward_to_llm_streamingå‡½æ•°
        async for chunk in forward_to_llm_streaming(original_messages, api_key, model):
            chunk_count += 1
            
            # æ£€æŸ¥chunkç»“æ„
            if hasattr(chunk, 'choices') and chunk.choices:
                delta = chunk.choices[0].delta
                
                # è·å–å†…å®¹
                if hasattr(delta, 'content') and delta.content:
                    content = delta.content
                    full_content += content
                    
                    # å®æ—¶æ‰“å°å†…å®¹ï¼ˆä¸æ¢è¡Œï¼‰
                    print(content, end='', flush=True)
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                if hasattr(chunk.choices[0], 'finish_reason') and chunk.choices[0].finish_reason:
                    print(f"\n\nğŸ æµå¼å“åº”ç»“æŸï¼ŒåŸå› : {chunk.choices[0].finish_reason}")
                    break
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        duration = time.time() - start_time
        
        print("\n" + "="*50)
        print("ğŸ“Š æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - å¤„ç†chunkæ•°: {chunk_count}")
        print(f"   - æ€»å†…å®¹é•¿åº¦: {len(full_content)} å­—ç¬¦")
        print(f"   - æ‰§è¡Œæ—¶é—´: {duration:.2f} ç§’")
        
        # æ£€æŸ¥å†…å®¹æ ¼å¼
        if "<think>" in full_content and "</think>" in full_content:
            print("âœ… ç¡®è®¤åŒ…å«æ¨ç†å†…å®¹æ ‡ç­¾")
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°æ¨ç†å†…å®¹æ ‡ç­¾")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨forward_to_llm_streamingå•å…ƒæµ‹è¯•")
    asyncio.run(test_forward_llm_streaming())