#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„æµå¼è¾“å‡ºåŠŸèƒ½
"""
import asyncio
import sys
import os

# Add project root to Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '.'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

async def test_streaming_fix():
    """æµ‹è¯•ä¿®å¤åçš„scenarioå·¥ä½œæµæµå¼è¾“å‡º"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„æµå¼è¾“å‡ºåŠŸèƒ½...")
    
    try:
        from config.manager import settings
        from src.scenario.manager import scenario_manager
        from utils.format_converter import convert_langgraph_chunk_to_sse
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_messages = [
            {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªé­”æ³•å¸ˆå­¦å¾’"},
        ]
        
        workflow_input = {
            "current_scenario": "è¿™æ˜¯ä¸€ä¸ªé­”æ³•å­¦é™¢çš„åœºæ™¯",
            "messages": test_messages,
            "original_messages": test_messages,
            "api_key": settings.agent.api_key,
            "model": settings.agent.model,
            "stream": True,
            "request_id": "test-streaming-fix"
        }
        
        print("ğŸ“¡ å¼€å§‹æµ‹è¯•scenarioå·¥ä½œæµæµå¼è¾“å‡º...")
        
        # æµ‹è¯•scenario_managerçš„æµå¼è¾“å‡º
        event_count = 0
        chat_model_events = 0
        valid_sse_chunks = 0
        
        async for event in scenario_manager.update_scenario_streaming(workflow_input):
            event_count += 1
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºChatOpenAIçš„æµå¼äº‹ä»¶
            if (event.get("event") == "on_chat_model_stream" and 
                event.get("name") == "ChatOpenAI" and
                event.get("data", {}).get("chunk")):
                
                chat_model_events += 1
                chunk = event["data"]["chunk"]
                
                print(f"ğŸ“¨ ChatOpenAIäº‹ä»¶ #{chat_model_events}:")
                print(f"   - Chunkç±»å‹: {type(chunk)}")
                print(f"   - Chunkå†…å®¹: {getattr(chunk, 'content', 'N/A')}")
                
                # æµ‹è¯•æ–°çš„è½¬æ¢å‡½æ•°
                sse_chunk = convert_langgraph_chunk_to_sse(chunk, workflow_input["model"], workflow_input["request_id"])
                if sse_chunk:
                    valid_sse_chunks += 1
                    print(f"   - SSEè½¬æ¢æˆåŠŸ: {sse_chunk[:100]}...")
                else:
                    print(f"   - SSEè½¬æ¢å¤±è´¥")
                
                print()
            
            # æ·»åŠ è¶…æ—¶ä¿æŠ¤
            if event_count > 100:
                print("â° è¾¾åˆ°äº‹ä»¶æ•°é‡é™åˆ¶ï¼Œåœæ­¢æµ‹è¯•")
                break
        
        print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"   - æ€»äº‹ä»¶æ•°: {event_count}")
        print(f"   - ChatOpenAIæµå¼äº‹ä»¶æ•°: {chat_model_events}")
        print(f"   - æˆåŠŸè½¬æ¢çš„SSEå—æ•°: {valid_sse_chunks}")
        
        if chat_model_events > 0 and valid_sse_chunks > 0:
            print("âœ… æµå¼è¾“å‡ºä¿®å¤æˆåŠŸï¼")
        elif chat_model_events > 0:
            print("âš ï¸  å‘ç°ChatOpenAIäº‹ä»¶ï¼Œä½†SSEè½¬æ¢å¤±è´¥")
        else:
            print("âŒ æ²¡æœ‰å‘ç°ChatOpenAIæµå¼äº‹ä»¶")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµå¼è¾“å‡ºä¿®å¤æµ‹è¯•")
    asyncio.run(test_streaming_fix())