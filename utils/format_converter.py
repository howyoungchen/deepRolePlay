"""
æ ¼å¼è½¬æ¢å·¥å…·ï¼šå°†LangGraphæ¶ˆæ¯è½¬æ¢ä¸ºOpenAIæ ¼å¼
"""
import json
import time
import uuid
from typing import Dict, Any, Optional
from langchain_core.messages import BaseMessage, AIMessage


def convert_to_openai_format(msg: BaseMessage, metadata: Optional[Dict] = None, model: str = "deepseek-chat") -> Dict[str, Any]:
    """
    å°†LangGraphæ¶ˆæ¯è½¬æ¢ä¸ºOpenAI SSEæ ¼å¼
    
    Args:
        msg: LangChainæ¶ˆæ¯å¯¹è±¡
        metadata: å¯é€‰çš„å…ƒæ•°æ®
        model: æ¨¡å‹åç§°
    
    Returns:
        OpenAIæ ¼å¼çš„å­—å…¸
    """
    # æå–å†…å®¹
    content = ""
    if hasattr(msg, 'content'):
        content = msg.content
    elif isinstance(msg, dict):
        content = msg.get('content', '')
    
    # æ„å»ºOpenAIæ ¼å¼å“åº”
    return {
        "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [{
            "index": 0,
            "delta": {
                "content": content,
                "role": "assistant" if isinstance(msg, AIMessage) else None
            },
            "finish_reason": None
        }],
        "usage": None
    }


def convert_to_openai_sse(msg: BaseMessage, metadata: Optional[Dict] = None, model: str = "deepseek-chat") -> str:
    """
    å°†LangGraphæ¶ˆæ¯è½¬æ¢ä¸ºOpenAI SSEæ ¼å¼çš„å­—ç¬¦ä¸²
    
    Args:
        msg: LangChainæ¶ˆæ¯å¯¹è±¡
        metadata: å¯é€‰çš„å…ƒæ•°æ®
        model: æ¨¡å‹åç§°
    
    Returns:
        SSEæ ¼å¼çš„å­—ç¬¦ä¸²
    """
    openai_chunk = convert_to_openai_format(msg, metadata, model)
    return f"data: {json.dumps(openai_chunk, ensure_ascii=False)}\n\n"


def create_done_message() -> str:
    """
    åˆ›å»ºSSEæµç»“æŸæ¶ˆæ¯
    
    Returns:
        SSEæ ¼å¼çš„DONEæ¶ˆæ¯
    """
    return "data: [DONE]\n\n"


def convert_final_response(response: BaseMessage, model: str = "deepseek-chat", stream: bool = False) -> Dict[str, Any]:
    """
    å°†æœ€ç»ˆçš„LLMå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼
    
    Args:
        response: LLMå“åº”
        model: æ¨¡å‹åç§°
        stream: æ˜¯å¦ä¸ºæµå¼å“åº”
    
    Returns:
        OpenAIæ ¼å¼çš„å®Œæ•´å“åº”
    """
    content = ""
    if hasattr(response, 'content'):
        content = response.content
    elif isinstance(response, dict):
        content = response.get('content', '')
    elif isinstance(response, str):
        content = response
    
    if stream:
        # æµå¼å“åº”æ ¼å¼
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
    else:
        # éæµå¼å“åº”æ ¼å¼
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 0,  # å¯ä»¥åœ¨å®é™…ä½¿ç”¨æ—¶è®¡ç®—
                "completion_tokens": 0,  # å¯ä»¥åœ¨å®é™…ä½¿ç”¨æ—¶è®¡ç®—
                "total_tokens": 0
            }
        }


def extract_content_from_event(event: Dict[str, Any]) -> Optional[str]:
    """
    ä»å·¥ä½œæµäº‹ä»¶ä¸­æå–å†…å®¹
    
    Args:
        event: å·¥ä½œæµäº‹ä»¶
    
    Returns:
        æå–çš„å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
    """
    # å°è¯•ä»ä¸åŒçš„äº‹ä»¶ç±»å‹ä¸­æå–å†…å®¹
    if 'messages' in event:
        messages = event['messages']
        if messages and len(messages) > 0:
            last_msg = messages[-1]
            if hasattr(last_msg, 'content'):
                return last_msg.content
            elif isinstance(last_msg, dict):
                return last_msg.get('content')
    
    if 'chunk' in event:
        chunk = event['chunk']
        if hasattr(chunk, 'content'):
            return chunk.content
        elif isinstance(chunk, dict):
            return chunk.get('content')
    
    if 'data' in event:
        data = event['data']
        if isinstance(data, str):
            return data
        elif isinstance(data, dict):
            return data.get('content') or data.get('output')
    
    return None
def convert_chunk_to_sse(chunk: Any, model: str, request_id: str) -> Optional[str]:
    """
    å°†ä»LLMç›´æ¥è·å–çš„æµå¼chunkè½¬æ¢ä¸ºOpenAI SSEæ ¼å¼
    
    Args:
        chunk: LLMçš„æµå¼å“åº”å—
        model: æ¨¡å‹åç§°
        request_id: è¯·æ±‚ID
        
    Returns:
        SSEæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œå¦‚æœchunkæ— æ•ˆåˆ™è¿”å›None
    """
    if not hasattr(chunk, 'choices') or not chunk.choices:
        return None
        
    delta = chunk.choices[0].delta
    
    # æå–å†…å®¹
    content = ""
    if hasattr(delta, 'content') and delta.content:
        content = delta.content
    
    # æå–æ¨ç†å†…å®¹
    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
        content = delta.reasoning_content

    if not content:
        return None

    sse_data = {
        "id": f"chatcmpl-{request_id}",
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [{
            "index": 0,
            "delta": {
                "role": "assistant",
                "content": content
            },
            "finish_reason": None
        }]
    }
    
    return f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"
def convert_chunk_to_sse_manual(content: str, model: str, request_id: str) -> str:
    """
    æ‰‹åŠ¨åˆ›å»ºåŒ…å«æŒ‡å®šå†…å®¹çš„SSEå—
    """
    sse_data = {
        "id": f"chatcmpl-{request_id}",
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [{
            "index": 0,
            "delta": {
                "role": "assistant",
                "content": content
            },
            "finish_reason": None
        }]
    }
    return f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"


def convert_langgraph_chunk_to_sse(chunk: Any, model: str, request_id: str) -> Optional[str]:
    """
    å°†LangGraphçš„AIMessageChunkè½¬æ¢ä¸ºOpenAI SSEæ ¼å¼
    
    Args:
        chunk: LangGraphçš„AIMessageChunkå¯¹è±¡
        model: æ¨¡å‹åç§°
        request_id: è¯·æ±‚ID
        
    Returns:
        SSEæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œå¦‚æœchunkæ— æ•ˆæˆ–å†…å®¹ä¸ºç©ºåˆ™è¿”å›None
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºAIMessageChunkå¹¶æå–å†…å®¹
    content = ""
    if hasattr(chunk, 'content'):
        content = chunk.content or ""
    elif isinstance(chunk, dict) and 'content' in chunk:
        content = chunk['content'] or ""
    
    # åªæœ‰å½“contentæœ‰å®é™…å†…å®¹æ—¶æ‰å‘é€SSE
    # è·³è¿‡ç©ºå†…å®¹çš„chunkä»¥å‡å°‘æ— ç”¨çš„ç½‘ç»œä¼ è¾“
    if not content or content.strip() == "":
        return None

    sse_data = {
        "id": f"chatcmpl-{request_id}",
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [{
            "index": 0,
            "delta": {
                "role": "assistant",
                "content": content
            },
            "finish_reason": None
        }]
    }
    
    return f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"


def convert_workflow_event_to_sse(event: Dict[str, Any], model: str, request_id: str) -> Optional[str]:
    """
    å°†å·¥ä½œæµäº‹ä»¶è½¬æ¢ä¸ºSSEæ ¼å¼ï¼Œæ”¯æŒå¤šç§äº‹ä»¶ç±»å‹
    åŸºäºpretty_print.pyçš„é€»è¾‘ï¼Œå°†å·¥å…·è°ƒç”¨ã€å·¥å…·è¾“å‡ºã€LLMè¾“å‡ºç­‰éƒ½è½¬ä¸ºSSEæ ¼å¼
    
    Args:
        event: å·¥ä½œæµäº‹ä»¶
        model: æ¨¡å‹åç§°
        request_id: è¯·æ±‚ID
        
    Returns:
        SSEæ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œå¦‚æœäº‹ä»¶ä¸éœ€è¦è¾“å‡ºåˆ™è¿”å›None
    """
    event_type = event.get("event", "unknown")
    name = event.get("name", "")
    data = event.get("data", {})
    
    # 1. å¤„ç†LLMæµå¼è¾“å‡º
    if event_type == "on_chat_model_stream" and name == "ChatOpenAI":
        chunk = data.get("chunk", {})
        if hasattr(chunk, 'content') and chunk.content and chunk.content.strip():
            sse_data = {
                "id": f"chatcmpl-{request_id}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "delta": {
                        "role": "assistant",
                        "content": chunk.content
                    },
                    "finish_reason": None
                }]
            }
            return f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"
    
    # 2. å¤„ç†èŠ‚ç‚¹å¼€å§‹
    elif event_type == "on_chain_start" and name in ["memory_flashback", "scenario_updater"]:
        content = f"\n{'='*50}\nğŸ”„ å¼€å§‹æ‰§è¡Œ {name} èŠ‚ç‚¹\n{'='*50}\n"
        sse_data = {
            "id": f"chatcmpl-{request_id}",
            "object": "chat.completion.chunk", 
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": None
            }]
        }
        return f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"
    
    # 3. å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹
    elif event_type == "on_tool_start":
        tool_name = name
        tool_input = data.get("input", {})
        
        content = f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}\n"
        if tool_input:
            content += "å‚æ•°:\n"
            for key, value in tool_input.items():
                # é™åˆ¶å‚æ•°å€¼çš„é•¿åº¦ä»¥é¿å…è¿‡é•¿çš„è¾“å‡º
                value_str = str(value)
                if len(value_str) > 100:
                    value_str = value_str[:100] + "..."
                content += f"  {key}: {value_str}\n"
        
        sse_data = {
            "id": f"chatcmpl-{request_id}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "role": "assistant", 
                    "content": content
                },
                "finish_reason": None
            }]
        }
        return f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"
    
    # 4. å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ
    elif event_type == "on_tool_end":
        tool_name = name
        tool_output = data.get("output", "")
        
        # æ·»åŠ åˆ†å‰²çº¿ï¼Œç„¶åæ˜¾ç¤ºå·¥å…·ç»“æœ
        content = f"{'-'*30}\n"
        
        # ç‰¹æ®Šå¤„ç†sequential_thinkingå·¥å…·
        if tool_name == "sequential_thinking":
            try:
                if hasattr(tool_output, 'content'):
                    output_content = tool_output.content
                elif isinstance(tool_output, str):
                    output_content = tool_output
                else:
                    output_content = str(tool_output)
                
                result = json.loads(output_content)
                thought_num = result.get("thought_number", "?")
                total_thoughts = result.get("total_thoughts", "?")
                
                content += f"ğŸ’­ æ€è€ƒæ­¥éª¤ {thought_num}/{total_thoughts} å®Œæˆ\n"
            except:
                content += f"ğŸ’­ {tool_name} å·¥å…·æ‰§è¡Œå®Œæˆ\n"
        else:
            # å…¶ä»–å·¥å…·æ˜¾ç¤ºè¾“å‡ºç»“æœ
            output_str = str(tool_output)
            if len(output_str) > 200:
                output_str = output_str[:200] + "..."
            content += f"âœ… {tool_name} ç»“æœ:\n{output_str}\n"
        
        content += "\n"
        
        sse_data = {
            "id": f"chatcmpl-{request_id}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": None
            }]
        }
        return f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"
    
    # 5. å¤„ç†èŠ‚ç‚¹å®Œæˆ
    elif event_type == "on_chain_end" and name in ["memory_flashback", "scenario_updater"]:
        content = f"\nâœ… {name} èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ\n{'='*50}\n\n"
        sse_data = {
            "id": f"chatcmpl-{request_id}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": None
            }]
        }
        return f"data: {json.dumps(sse_data, ensure_ascii=False)}\n\n"
    
    return None