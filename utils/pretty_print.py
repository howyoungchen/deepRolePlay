"""
Pretty print tool, supports formatted display of LangGraph stream events
"""


def pretty_print_stream_events(event):
    """
    Pretty prints LangGraph stream events.
    
    Args:
        event: The event dictionary from astream_events.
    """
    event_type = event.get("event", "unknown")
    name = event.get("name", "")
    data = event.get("data", {})
    
    # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰äº‹ä»¶ç±»åž‹ä»¥æŸ¥çœ‹ç¼ºå¤±çš„AIæ¶ˆæ¯äº‹ä»¶
    # print(f"DEBUG: {event_type} - {name}")
    
    # Global state tracking (using function attributes)
    if not hasattr(pretty_print_stream_events, 'current_node'):
        pretty_print_stream_events.current_node = None
    if not hasattr(pretty_print_stream_events, 'message_buffer'):
        pretty_print_stream_events.message_buffer = ""
    if not hasattr(pretty_print_stream_events, 'ai_message_started'):
        pretty_print_stream_events.ai_message_started = False
    
    # Detect node start
    if event_type == "on_chain_start" and name in ["memory_flashback", "scenario_updater", "llm_forwarding"]:
        pretty_print_stream_events.current_node = name
        # å¯¹llm_forwardingèŠ‚ç‚¹ä¸æ˜¾ç¤ºå¼€å§‹ä¿¡æ¯
        if name != "llm_forwarding":
            print(f"\nðŸ”„ Update from node {name}:")
            print()
        return
    
    # Handle AI message stream output
    if event_type == "on_chat_model_stream" and name == "ChatOpenAI" and pretty_print_stream_events.current_node:
        chunk = data.get("chunk", {})
        if hasattr(chunk, 'content'):
            if not pretty_print_stream_events.ai_message_started:
                # å¯¹llm_forwardingèŠ‚ç‚¹ä¸æ˜¾ç¤ºAIæ¶ˆæ¯æ ‡é¢˜
                if pretty_print_stream_events.current_node != "llm_forwarding":
                    print("================================== Ai Message ==================================")
                    print(f"Name: {pretty_print_stream_events.current_node}_agent")
                pretty_print_stream_events.ai_message_started = True
            
            # åªæœ‰å½“contentä¸ä¸ºç©ºæ—¶æ‰è¾“å‡ºå’Œç´¯ç§¯
            if chunk.content:
                # Accumulate message content
                pretty_print_stream_events.message_buffer += chunk.content
                print(chunk.content, end="", flush=True)
        return
    
    # Newline at the end of the AI message
    if event_type == "on_chat_model_end" and name == "ChatOpenAI" and pretty_print_stream_events.current_node:
        if pretty_print_stream_events.ai_message_started:
            print("\n")
            pretty_print_stream_events.ai_message_started = False
            pretty_print_stream_events.message_buffer = ""
        return
    
    # Detect tool call start
    if event_type == "on_tool_start" and pretty_print_stream_events.current_node:
        tool_name = name
        tool_input = data.get("input", {})
        
        # If there is an AI message buffer, end it first
        if pretty_print_stream_events.ai_message_started:
            print("\n")
            pretty_print_stream_events.ai_message_started = False
        
        print("Tool Calls:")
        print(f"  {tool_name}")  
        if tool_input:
            print("  Args:")
            for key, value in tool_input.items():
                print(f"    {key}: {value}")
        print()
        return
    
    # Detect tool call end
    if event_type == "on_tool_end" and pretty_print_stream_events.current_node:
        tool_name = name
        tool_output = data.get("output", "")
        
        # å¯¹sequential_thinkingå·¥å…·çš„è¾“å‡ºè¿›è¡Œç‰¹æ®Šå¤„ç†
        if tool_name == "sequential_thinking":
            # sequential_thinkingå·²ç»æœ‰è‡ªå·±çš„å¯è§†åŒ–è¾“å‡ºï¼ˆæ¡†æ¡†ï¼‰ï¼Œè¿™é‡Œåªæ˜¾ç¤ºç®€åŒ–çš„ç»“æžœ
            try:
                import json
                # æ£€æŸ¥tool_outputæ˜¯å¦æœ‰contentå±žæ€§
                if hasattr(tool_output, 'content'):
                    content = tool_output.content
                elif isinstance(tool_output, str):
                    content = tool_output
                else:
                    content = str(tool_output)
                
                result = json.loads(content)
                success = result.get("success", False)
                thought_num = result.get("thought_number", "?")
                total_thoughts = result.get("total_thoughts", "?")
                next_needed = result.get("next_thought_needed", False)
                history_length = result.get("thought_history_length", "?")
                
                print(f"Tool Results:")
                print(f"  sequential_thinking")
                print(f"  Returns:")
                print(f"    success: {str(success).lower()}")
                print(f"    thought_number: {thought_num}")
                print(f"    total_thoughts: {total_thoughts}")
                print(f"    next_thought_needed: {str(next_needed).lower()}")
                print(f"    thought_history_length: {history_length}")
            except Exception as e:
                print(f"Tool Results:")
                print(f"  sequential_thinking")
                print(f"  Returns: {tool_output}")
                print(f"  (Error parsing: {e})")
        else:
            # å…¶ä»–å·¥å…·ä¿æŒåŽŸæœ‰çš„è¯¦ç»†æ˜¾ç¤º
            print(f"\nðŸ”§ Update from node tools:")
            print()
            print("================================= Tool Message =================================")
            print(f"Name: {tool_name}")
            print()
            
            if isinstance(tool_output, str):
                if len(tool_output) > 500:
                    print(f"{tool_output[:500]}... (truncated)")
                else:
                    print(tool_output)
            else:
                print(tool_output)
            print()
        return
    
    # Detect node completion
    if event_type == "on_chain_end" and name in ["memory_flashback", "scenario_updater", "llm_forwarding"]:
        node_output = data.get("output", {})
        
        # If there is an AI message buffer, end it first
        if pretty_print_stream_events.ai_message_started:
            print("\n")
            pretty_print_stream_events.ai_message_started = False
        
        # å¯¹llm_forwardingèŠ‚ç‚¹åšç‰¹æ®Šå¤„ç†ï¼Œä¸æ˜¾ç¤ºæŠ€æœ¯ç»†èŠ‚
        if name == "llm_forwarding":
            pretty_print_stream_events.current_node = None
            return
        
        print(f"âœ… Node {name} completed:")
        for key, value in node_output.items():
            if isinstance(value, str) and len(value) > 100:
                print(f"  {key}: {value[:100]}... (truncated)")
            else:
                print(f"  {key}: {value}")
        print("-" * 80)
        pretty_print_stream_events.current_node = None
        return


