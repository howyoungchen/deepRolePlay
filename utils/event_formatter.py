"""
äº‹ä»¶æ ¼å¼åŒ–å™¨ï¼šå°†LangGraphäº‹ä»¶è½¬æ¢ä¸ºç”¨æˆ·å‹å¥½çš„SSEæ ¼å¼
åŸºäºpretty_printçš„é€»è¾‘ï¼Œä½†è¾“å‡ºä¸ºOpenAIå…¼å®¹çš„SSEæµ
"""
import json
import time
import uuid
from typing import Dict, Any, Optional


class EventFormatter:
    """LangGraphäº‹ä»¶çš„SSEæ ¼å¼åŒ–å™¨"""
    
    def __init__(self, model: str = "deepseek-chat"):
        self.model = model
        self.current_node = None
        self.message_buffer = ""
        self.ai_message_started = False
        
    def create_sse_chunk(self, content: str) -> str:
        """åˆ›å»ºSSEæ ¼å¼çš„æ•°æ®å—"""
        chunk_data = {
            "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": self.model,
            "choices": [{
                "index": 0,
                "delta": {
                    "content": content,
                    "role": "assistant"
                },
                "finish_reason": None
            }]
        }
        return f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
    
    def format_event_to_sse(self, event: Dict[str, Any]) -> Optional[str]:
        """
        å°†å•ä¸ªLangGraphäº‹ä»¶æ ¼å¼åŒ–ä¸ºSSEæ ¼å¼
        åŸºäºpretty_print_stream_eventsçš„é€»è¾‘
        """
        event_type = event.get("event", "unknown")
        name = event.get("name", "")
        data = event.get("data", {})
        
        # æ£€æµ‹èŠ‚ç‚¹å¼€å§‹
        if event_type == "on_chain_start" and name in ["memory_flashback", "scenario_updater", "llm_forwarding"]:
            self.current_node = name
            # å¯¹llm_forwardingèŠ‚ç‚¹ä¸æ˜¾ç¤ºå¼€å§‹ä¿¡æ¯
            if name != "llm_forwarding":
                content = f"\nğŸ”„ Update from node {name}:\n\n"
                return self.create_sse_chunk(content)
            return None
        
        # å¤„ç†AIæ¶ˆæ¯æµè¾“å‡º
        if event_type == "on_chat_model_stream" and name == "ChatOpenAI" and self.current_node:
            chunk = data.get("chunk", {})
            if hasattr(chunk, 'content'):
                if not self.ai_message_started:
                    # å¯¹llm_forwardingèŠ‚ç‚¹ä¸æ˜¾ç¤ºAIæ¶ˆæ¯æ ‡é¢˜
                    if self.current_node != "llm_forwarding":
                        header = "================================== Ai Message ==================================\n"
                        header += f"Name: {self.current_node}_agent\n\n"
                        self.ai_message_started = True
                        return self.create_sse_chunk(header)
                    else:
                        self.ai_message_started = True
                
                # åªæœ‰å½“contentä¸ä¸ºç©ºæ—¶æ‰è¾“å‡ºå’Œç´¯ç§¯
                if chunk.content:
                    # ç´¯ç§¯æ¶ˆæ¯å†…å®¹
                    self.message_buffer += chunk.content
                    return self.create_sse_chunk(chunk.content)
            return None
        
        # AIæ¶ˆæ¯ç»“æŸæ—¶çš„æ¢è¡Œ
        if event_type == "on_chat_model_end" and name == "ChatOpenAI" and self.current_node:
            if self.ai_message_started:
                self.ai_message_started = False
                self.message_buffer = ""
                if self.current_node != "llm_forwarding":
                    return self.create_sse_chunk("\n")
            return None
        
        # æ£€æµ‹å·¥å…·è°ƒç”¨å¼€å§‹
        if event_type == "on_tool_start" and self.current_node:
            tool_name = name
            tool_input = data.get("input", {})
            
            # å¦‚æœæœ‰AIæ¶ˆæ¯ç¼“å†²åŒºï¼Œå…ˆç»“æŸå®ƒ
            if self.ai_message_started:
                self.ai_message_started = False
                
            content = "\nTool Calls:\n"
            content += f"  {tool_name}\n"
            if tool_input:
                content += "  Args:\n"
                for key, value in tool_input.items():
                    content += f"    {key}: {value}\n"
            content += "\n"
            return self.create_sse_chunk(content)
        
        # æ£€æµ‹å·¥å…·è°ƒç”¨ç»“æŸ
        if event_type == "on_tool_end" and self.current_node:
            tool_name = name
            tool_output = data.get("output", "")
            
            # å¯¹sequential_thinkingå·¥å…·çš„è¾“å‡ºè¿›è¡Œç‰¹æ®Šå¤„ç†
            if tool_name == "sequential_thinking":
                try:
                    # æ£€æŸ¥tool_outputæ˜¯å¦æœ‰contentå±æ€§
                    if hasattr(tool_output, 'content'):
                        content_str = tool_output.content
                    elif isinstance(tool_output, str):
                        content_str = tool_output
                    else:
                        content_str = str(tool_output)
                    
                    result = json.loads(content_str)
                    success = result.get("success", False)
                    thought_num = result.get("thought_number", "?")
                    total_thoughts = result.get("total_thoughts", "?")
                    next_needed = result.get("next_thought_needed", False)
                    history_length = result.get("thought_history_length", "?")
                    
                    content = f"Tool Results:\n"
                    content += f"  sequential_thinking\n"
                    content += f"  Returns:\n"
                    content += f"    success: {str(success).lower()}\n"
                    content += f"    thought_number: {thought_num}\n"
                    content += f"    total_thoughts: {total_thoughts}\n"
                    content += f"    next_thought_needed: {str(next_needed).lower()}\n"
                    content += f"    thought_history_length: {history_length}\n"
                    
                    return self.create_sse_chunk(content)
                except Exception as e:
                    content = f"Tool Results:\n"
                    content += f"  sequential_thinking\n"
                    content += f"  Returns: {tool_output}\n"
                    content += f"  (Error parsing: {e})\n"
                    return self.create_sse_chunk(content)
            else:
                # å…¶ä»–å·¥å…·ä¿æŒåŸæœ‰çš„è¯¦ç»†æ˜¾ç¤º
                content = f"\nğŸ”§ Update from node tools:\n\n"
                content += "================================= Tool Message =================================\n"
                content += f"Name: {tool_name}\n\n"
                
                if isinstance(tool_output, str):
                    if len(tool_output) > 500:
                        content += f"{tool_output[:500]}... (truncated)\n"
                    else:
                        content += f"{tool_output}\n"
                else:
                    content += f"{tool_output}\n"
                content += "\n"
                return self.create_sse_chunk(content)
        
        # æ£€æµ‹èŠ‚ç‚¹å®Œæˆ
        if event_type == "on_chain_end" and name in ["memory_flashback", "scenario_updater", "llm_forwarding"]:
            node_output = data.get("output", {})
            
            # å¦‚æœæœ‰AIæ¶ˆæ¯ç¼“å†²åŒºï¼Œå…ˆç»“æŸå®ƒ
            if self.ai_message_started:
                self.ai_message_started = False
                
            # å¯¹llm_forwardingèŠ‚ç‚¹åšç‰¹æ®Šå¤„ç†ï¼Œä¸æ˜¾ç¤ºæŠ€æœ¯ç»†èŠ‚
            if name == "llm_forwarding":
                self.current_node = None
                return None
            
            content = f"âœ… Node {name} completed:\n"
            for key, value in node_output.items():
                if isinstance(value, str) and len(value) > 100:
                    content += f"  {key}: {value[:100]}... (truncated)\n"
                else:
                    content += f"  {key}: {value}\n"
            content += "-" * 80 + "\n"
            self.current_node = None
            return self.create_sse_chunk(content)
        
        return None