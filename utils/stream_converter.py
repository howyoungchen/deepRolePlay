"""
æµå¼äº‹ä»¶è½¬æ¢å·¥å…·
å°†LangGraphå·¥ä½œæµäº‹ä»¶è½¬æ¢ä¸ºOpenAIå…¼å®¹çš„SSEæ ¼å¼
"""
import json
import time
import uuid
from typing import Dict, Any, AsyncGenerator


class WorkflowStreamConverter:
    """å·¥ä½œæµæµå¼äº‹ä»¶è½¬æ¢å™¨"""
    
    def __init__(self, request_id: str = None):
        self.request_id = request_id or str(uuid.uuid4())
        self.created_time = int(time.time())
        self.current_node = None
        self.message_buffer = ""
        self.ai_message_started = False
    
    def create_sse_data(self, content: str, event_type: str = "workflow") -> str:
        """åˆ›å»ºSSEæ ¼å¼çš„æ•°æ®"""
        chunk_data = {
            "id": f"chatcmpl-{self.request_id}",
            "object": "chat.completion.chunk",
            "created": self.created_time,
            "model": "DeepRolePlay-workflow",
            "choices": [{
                "index": 0,
                "delta": {
                    "content": content,
                    "role": "assistant" if event_type == "workflow" else "system"
                },
                "finish_reason": None
            }],
            "workflow_event": True,
            "event_type": event_type
        }
        return f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
    
    def create_workflow_done_event(self) -> str:
        """åˆ›å»ºå·¥ä½œæµå®Œæˆäº‹ä»¶"""
        chunk_data = {
            "id": f"chatcmpl-{self.request_id}",
            "object": "chat.completion.chunk", 
            "created": self.created_time,
            "model": "DeepRolePlay-workflow",
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "workflow_complete"
            }],
            "workflow_event": True,
            "event_type": "workflow_complete"
        }
        return f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
    
    async def convert_workflow_events(
        self, 
        workflow_events: AsyncGenerator[Dict[str, Any], None]
    ) -> AsyncGenerator[str, None]:
        """
        è½¬æ¢å·¥ä½œæµäº‹ä»¶ä¸ºSSEæµ
        
        Args:
            workflow_events: å·¥ä½œæµäº‹ä»¶å¼‚æ­¥ç”Ÿæˆå™¨
            
        Yields:
            SSEæ ¼å¼çš„å­—ç¬¦ä¸²
        """
        try:
            # å‘é€å·¥ä½œæµå¼€å§‹äº‹ä»¶
            yield self.create_sse_data("ğŸ”„ å¼€å§‹æ›´æ–°æƒ…æ™¯...\n\n", "workflow_start")
            
            async for event in workflow_events:
                sse_chunk = self._process_event(event)
                if sse_chunk:
                    yield sse_chunk
            
            # å‘é€å·¥ä½œæµå®Œæˆäº‹ä»¶
            yield self.create_sse_data("\nâœ… æƒ…æ™¯æ›´æ–°å®Œæˆï¼Œå¼€å§‹ç”Ÿæˆå›å¤...\n\n", "workflow_end")
            yield self.create_workflow_done_event()
            
        except Exception as e:
            error_msg = f"âŒ å·¥ä½œæµæ‰§è¡Œå‡ºé”™: {str(e)}\n\n"
            yield self.create_sse_data(error_msg, "workflow_error")
            yield self.create_workflow_done_event()
    
    def _process_event(self, event: Dict[str, Any]) -> str:
        """å¤„ç†å•ä¸ªå·¥ä½œæµäº‹ä»¶"""
        event_type = event.get("event", "unknown")
        name = event.get("name", "")
        data = event.get("data", {})
        
        # èŠ‚ç‚¹å¼€å§‹
        if event_type == "on_chain_start" and name in ["memory_flashback", "scenario_updater"]:
            self.current_node = name
            node_name_map = {
                "memory_flashback": "è®°å¿†é—ªå›",
                "scenario_updater": "æƒ…æ™¯æ›´æ–°"
            }
            content = f"ğŸ”„ å¼€å§‹æ‰§è¡Œ {node_name_map.get(name, name)} èŠ‚ç‚¹...\n"
            return self.create_sse_data(content, "node_start")
        
        # AIæ¶ˆæ¯æµå¼è¾“å‡º
        if event_type == "on_chat_model_stream" and self.current_node:
            chunk = data.get("chunk", {})
            if hasattr(chunk, 'content') and chunk.content:
                if not self.ai_message_started:
                    self.ai_message_started = True
                    header = f"\nğŸ’­ {self.current_node} æ€è€ƒä¸­:\n"
                    return self.create_sse_data(header, "ai_thinking")
                
                # è¿”å›AIæ€è€ƒå†…å®¹
                return self.create_sse_data(chunk.content, "ai_content")
        
        # AIæ¶ˆæ¯ç»“æŸ
        if event_type == "on_chat_model_end" and self.current_node and self.ai_message_started:
            self.ai_message_started = False
            return self.create_sse_data("\n", "ai_end")
        
        # å·¥å…·è°ƒç”¨å¼€å§‹
        if event_type == "on_tool_start" and self.current_node:
            tool_name = name
            tool_input = data.get("input", {})
            
            content = f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}\n"
            if tool_input:
                content += "å‚æ•°:\n"
                for key, value in tool_input.items():
                    # æˆªæ–­é•¿å†…å®¹
                    if isinstance(value, str) and len(value) > 100:
                        value = value[:100] + "..."
                    content += f"  {key}: {value}\n"
            content += "\n"
            
            return self.create_sse_data(content, "tool_start")
        
        # å·¥å…·è°ƒç”¨ç»“æŸ
        if event_type == "on_tool_end" and self.current_node:
            tool_name = name
            tool_output = data.get("output", "")
            
            content = f"âœ… å·¥å…· {tool_name} æ‰§è¡Œå®Œæˆ\n"
            if isinstance(tool_output, str):
                if len(tool_output) > 200:
                    content += f"è¾“å‡º: {tool_output[:200]}...\n"
                else:
                    content += f"è¾“å‡º: {tool_output}\n"
            content += "\n"
            
            return self.create_sse_data(content, "tool_end")
        
        # èŠ‚ç‚¹å®Œæˆ
        if event_type == "on_chain_end" and name in ["memory_flashback", "scenario_updater"]:
            node_output = data.get("output", {})
            
            node_name_map = {
                "memory_flashback": "è®°å¿†é—ªå›",
                "scenario_updater": "æƒ…æ™¯æ›´æ–°"
            }
            
            content = f"âœ… {node_name_map.get(name, name)} èŠ‚ç‚¹å®Œæˆ\n"
            for key, value in node_output.items():
                if isinstance(value, str) and len(value) > 100:
                    content += f"  {key}: {value[:100]}...\n"
                else:
                    content += f"  {key}: {value}\n"
            content += "\n" + "-" * 50 + "\n"
            
            self.current_node = None
            return self.create_sse_data(content, "node_end")
        
        return None


async def create_unified_stream(
    workflow_events: AsyncGenerator[Dict[str, Any], None],
    llm_stream: AsyncGenerator[str, None],
    request_id: str = None
) -> AsyncGenerator[str, None]:
    """
    åˆ›å»ºç»Ÿä¸€çš„æµå¼è¾“å‡ºï¼Œåˆå¹¶å·¥ä½œæµäº‹ä»¶å’ŒLLMå“åº”
    
    Args:
        workflow_events: å·¥ä½œæµäº‹ä»¶æµ
        llm_stream: LLMå“åº”æµ  
        request_id: è¯·æ±‚ID
        
    Yields:
        ç»Ÿä¸€çš„SSEæ ¼å¼æµ
    """
    converter = WorkflowStreamConverter(request_id)
    
    # å…ˆè¾“å‡ºå·¥ä½œæµäº‹ä»¶
    async for sse_chunk in converter.convert_workflow_events(workflow_events):
        yield sse_chunk
    
    # å†è¾“å‡ºLLMå“åº”
    async for llm_chunk in llm_stream:
        yield llm_chunk