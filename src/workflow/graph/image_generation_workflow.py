"""
å›¾ç‰‡ç”Ÿæˆå·¥ä½œæµ - åŸºäºæƒ…æ™¯ç”Ÿæˆå›¾ç‰‡
"""
import asyncio
import sys
import os
from typing import Dict, Any, List, Optional
from typing_extensions import TypedDict
from pathlib import Path

# Add project root to Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI

from config.manager import settings
# å·²ç§»é™¤workflow_loggerï¼Œä½¿ç”¨ç®€å•çš„æ—¥å¿—ä¿å­˜
from src.workflow.tools.image_generation_tool import generate_one_img
from src.workflow.tools.scenario_table_tools import scenario_manager
from src.prompts.image_generation_prompts import IMAGE_SYSTEM_PROMPT, IMAGE_USER_PROMPT_TEMPLATE

# æ¨¡å—çº§åˆå§‹åŒ–scenario_manager
scenario_manager.init(settings.scenario.file_path)


class ImageGenerationState(TypedDict):
    """å›¾ç‰‡ç”Ÿæˆå·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    # è¾“å…¥
    current_scenario: str
    
    # ä¸­é—´çŠ¶æ€
    image_prompt: str
    tool_calls: List[Dict[str, Any]]
    
    # è¾“å‡º
    generated_image_paths: List[str]


async def init_scenario_node(state: ImageGenerationState) -> Dict[str, Any]:
    """åˆå§‹åŒ–èŠ‚ç‚¹ï¼šè¯»å–è¡¨æ ¼å†…å®¹"""
    print("ğŸš€ Scenario table initialization...", flush=True)
    
    import time
    start_time = time.time()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºæ—¥å¿—
    inputs = {
        "node_type": "init_scenario_image",
        "input_state_keys": list(state.keys()) if state else []
    }
    
    try:
        # ç›´æ¥ä»scenario_managerè¯»å–æ‰€æœ‰è¡¨æ ¼å†…å®¹
        current_scenario = scenario_manager.get_all_pretty_tables(description=True, operation_guide=False)
        
        duration = time.time() - start_time
        
        outputs = {
            "scenario_length": len(current_scenario),
            "scenario_preview": current_scenario[:200] + "..." if len(current_scenario) > 200 else current_scenario
        }
        
        # init_scenario_nodeä¸è°ƒç”¨LLMï¼Œæ— éœ€è®°å½•æ—¥å¿—
        
        print(f"  âœ“ Loaded scenario tables, total length: {len(current_scenario)}", flush=True)
        return {"current_scenario": current_scenario}
        
    except Exception as e:
        duration = time.time() - start_time
        import traceback
        error_details = traceback.format_exc()
        
        # é”™è¯¯ä¸è®°å½•æ—¥å¿—ï¼Œåªè¾“å‡ºåˆ°æ§åˆ¶å°
        
        print(f"  âŒ Scenario table initialization failed: {str(e)}", flush=True)
        return {"current_scenario": "Scenario tables not initialized or empty."}


async def llm_generate_prompt_node(state: ImageGenerationState) -> Dict[str, Any]:
    """LLMèŠ‚ç‚¹ï¼šæ ¹æ®æƒ…æ™¯ç”Ÿæˆå›¾ç‰‡æç¤ºè¯å¹¶è°ƒç”¨å·¥å…·"""
    print("ğŸ¨ Generating image prompt from scenario...", flush=True)
    
    import time
    start_time = time.time()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºæ—¥å¿—
    current_scenario = state.get("current_scenario", "")
    
    inputs = {
        "current_scenario_length": len(current_scenario),
        "num_images_requested": settings.comfyui.num_images,
        "model_temperature": 0.7,
        "current_scenario": current_scenario if current_scenario else "[Empty]"
    }
    
    try:
        # å¯¼å…¥ç»“æ„åŒ–å·¥å…·è¾…åŠ©å‡½æ•°
        from src.workflow.tools.structured_tool_helper import generate_tool_prompts, parse_tool_calls
        
        # åˆå§‹åŒ–æ¨¡å‹
        agent_config = settings.agent
        extra_body = {"provider": {"only": [agent_config.provider]}} if agent_config.provider else {}
        model = ChatOpenAI(
            model=agent_config.model,
            api_key=agent_config.api_key,
            base_url=agent_config.base_url,
            temperature=0.7,
            extra_body=extra_body
        )
        
        # ç”Ÿæˆå·¥å…·æç¤ºè¯
        tools_description_system, tools_description_user = generate_tool_prompts([generate_one_img])
        
        # ä½¿ç”¨æ–°çš„æç¤ºè¯æ¨¡æ¿
        system_prompt = IMAGE_SYSTEM_PROMPT.format(tools_description_system=tools_description_system)
        user_input = IMAGE_USER_PROMPT_TEMPLATE.format(
            current_scenario=current_scenario,
            num_images=settings.comfyui.num_images,
            tools_description_user=tools_description_user
        )
        
        print(f"  Scenario length: {len(current_scenario)}", flush=True)
        print(f"  Generating {settings.comfyui.num_images} images", flush=True)
        
        # è°ƒç”¨LLM
        response = await model.ainvoke([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ])
        
        # è§£æç»“æ„åŒ–è¾“å‡º
        response_text = response.content if hasattr(response, 'content') else str(response)
        tool_calls = parse_tool_calls(response_text)
        
        # æå–ç”Ÿæˆçš„æç¤ºè¯ï¼ˆä»å·¥å…·è°ƒç”¨å‚æ•°ä¸­ï¼‰
        image_prompt = ""
        tool_call_details = []
        
        for tool_call in tool_calls:
            tool_name = tool_call.get("tool_name", "")
            args = tool_call.get("arguments", {})
            if tool_name == "generate_one_img":
                prompt = args.get('positive_prompt', '')
                if not image_prompt:  # åªå–ç¬¬ä¸€ä¸ªä½œä¸ºä¸»è¦æç¤ºè¯
                    image_prompt = prompt
                
                tool_call_details.append({
                    "tool_name": tool_name,
                    "positive_prompt": prompt,
                    "positive_prompt_length": len(prompt)
                })
        
        # ä¿å­˜æ—¥å¿—
        from utils.simple_logger import save_log
        from datetime import datetime
        
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "node_type": "llm_image_generation",
            "model_config": {
                "model": agent_config.model,
                "base_url": agent_config.base_url,
                "temperature": 0.7,
                "provider": agent_config.provider if agent_config.provider else None
            },
            "model_input": {
                "system": system_prompt,
                "user": user_input
            },
            "model_output": response_text
        }
        
        log_file = f"./logs/workflow/{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}_image.json"
        save_log(log_file, log_data)
        
        print(f"  âœ“ Generated prompt: {image_prompt[:100]}..." if image_prompt else "  âŒ No prompt generated", flush=True)
        print(f"  âœ“ Tool calls: {len(tool_calls)}", flush=True)
        
        return {
            "image_prompt": image_prompt,
            "tool_calls": tool_calls
        }
        
    except Exception as e:
        duration = time.time() - start_time
        import traceback
        error_details = traceback.format_exc()
        
        # é”™è¯¯ä¸è®°å½•æ—¥å¿—ï¼Œåªè¾“å‡ºåˆ°æ§åˆ¶å°
        
        print(f"âŒ LLM prompt generation failed: {str(e)}", flush=True)
        return {
            "image_prompt": "",
            "tool_calls": []
        }


async def tool_execution_node(state: ImageGenerationState) -> Dict[str, Any]:
    """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ï¼šå¹¶è¡Œæ‰§è¡Œå›¾ç‰‡ç”Ÿæˆå·¥å…·"""
    print("ğŸ› ï¸ Executing image generation tools...", flush=True)
    
    import time
    start_time = time.time()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºæ—¥å¿—
    tool_calls = state.get("tool_calls", [])
    
    inputs = {
        "tool_calls_count": len(tool_calls),
        "tool_calls_details": [
            {
                "tool_name": tc.get("name", ""),
                "args_keys": list(tc.get("args", {}).keys()),
                "positive_prompt_length": len(tc.get("args", {}).get("positive_prompt", ""))
            } for tc in tool_calls
        ]
    }
    
    try:
        if not tool_calls:
            duration = time.time() - start_time
            
            outputs = {
                "generated_paths_count": 0,
                "execution_results": [],
                "error_message": "No tool calls to execute"
            }
            
            # tool_execution_nodeä¸è°ƒç”¨LLMï¼Œæ— éœ€è®°å½•æ—¥å¿—
            
            print("  âŒ No tool calls to execute", flush=True)
            return {"generated_image_paths": []}
        
        print(f"  Executing {len(tool_calls)} tool calls", flush=True)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        generated_paths = []
        execution_details = []
        
        from langchain_core.runnables import RunnableConfig
        config = RunnableConfig()
        
        # ä½¿ç”¨å¹¶å‘æ‰§è¡Œå¤šä¸ªå·¥å…·è°ƒç”¨
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = []
            
            for i, tool_call in enumerate(tool_calls):
                tool_name = tool_call.get("tool_name", "")
                args = tool_call.get("arguments", {})
                
                if tool_name == "generate_one_img":
                    positive_prompt = args.get("positive_prompt", "")
                    print(f"  [{i+1}] Generating image with prompt: {positive_prompt[:50]}...", flush=True)
                    
                    future = executor.submit(generate_one_img.invoke, {"positive_prompt": positive_prompt}, config)
                    futures.append((future, i, tool_call))
                else:
                    print(f"  [{i+1}] âŒ Unknown tool: {tool_name}", flush=True)
                    execution_details.append({
                        "index": i,
                        "tool_name": tool_name,
                        "status": "error",
                        "error": f"Unknown tool: {tool_name}",
                        "args": args
                    })
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future, i, tool_call in futures:
                tool_name = tool_call.get("tool_name", "")
                args = tool_call.get("arguments", {})
                
                try:
                    result = await asyncio.get_event_loop().run_in_executor(None, future.result)
                    generated_paths.append(result)
                    print(f"  [{i+1}] âœ“ Image generated: {result}", flush=True)
                    
                    execution_details.append({
                        "index": i,
                        "tool_name": tool_name,
                        "status": "success",
                        "result": result,
                        "args": args,
                        "positive_prompt": args.get("positive_prompt", "")[:100] + "..." if len(args.get("positive_prompt", "")) > 100 else args.get("positive_prompt", "")
                    })
                    
                except Exception as e:
                    error_msg = f"é”™è¯¯ï¼š{str(e)}"
                    generated_paths.append(error_msg)
                    print(f"  [{i+1}] âŒ Generation failed: {str(e)}", flush=True)
                    
                    execution_details.append({
                        "index": i,
                        "tool_name": tool_name,
                        "status": "error",
                        "error": str(e),
                        "args": args,
                        "positive_prompt": args.get("positive_prompt", "")[:100] + "..." if len(args.get("positive_prompt", "")) > 100 else args.get("positive_prompt", "")
                    })
        
        duration = time.time() - start_time
        
        outputs = {
            "generated_paths_count": len(generated_paths),
            "generated_paths": generated_paths,
            "execution_details": execution_details,
            "successful_generations": len([d for d in execution_details if d.get("status") == "success"]),
            "failed_generations": len([d for d in execution_details if d.get("status") == "error"])
        }
        
        # tool_execution_nodeä¸è°ƒç”¨LLMï¼Œæ— éœ€è®°å½•æ—¥å¿—
        
        print(f"  âœ“ Total generated: {len(generated_paths)} images", flush=True)
        return {"generated_image_paths": generated_paths}
            
    except Exception as e:
        duration = time.time() - start_time
        import traceback
        error_details = traceback.format_exc()
        
        outputs = {
            "generated_paths_count": 0,
            "execution_results": [],
            "error_message": str(e)
        }
        
        # é”™è¯¯ä¸è®°å½•æ—¥å¿—ï¼Œåªè¾“å‡ºåˆ°æ§åˆ¶å°
        
        print(f"âŒ Tool execution failed: {str(e)}", flush=True)
        return {"generated_image_paths": [f"é”™è¯¯ï¼š{str(e)}"]}


def create_image_generation_workflow():
    """åˆ›å»ºå›¾ç‰‡ç”Ÿæˆå·¥ä½œæµ"""
    builder = StateGraph(ImageGenerationState)
    
    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("init_scenario", init_scenario_node)
    builder.add_node("llm_generate_prompt", llm_generate_prompt_node)
    builder.add_node("tool_execution", tool_execution_node)
    
    # æ·»åŠ è¾¹ï¼šçº¿æ€§æµç¨‹
    builder.add_edge(START, "init_scenario")
    builder.add_edge("init_scenario", "llm_generate_prompt")
    builder.add_edge("llm_generate_prompt", "tool_execution")
    builder.add_edge("tool_execution", END)
    
    return builder.compile()


async def test_image_workflow():
    """æµ‹è¯•å›¾ç‰‡ç”Ÿæˆå·¥ä½œæµ"""
    print("ğŸ§ª Image generation workflow test starting...", flush=True)
    
    workflow = create_image_generation_workflow()
    
    test_input = {
        "current_scenario": ""  # ç©ºåœºæ™¯ï¼Œè®©initèŠ‚ç‚¹ä»è¡¨æ ¼åŠ è½½
    }
    
    try:
        result = await workflow.ainvoke(test_input)
        print(f"âœ… Image workflow test completed successfully", flush=True)
        print(f"Generated paths: {result.get('generated_image_paths', [])}", flush=True)
        
    except Exception as e:
        print(f"âŒ Image workflow test failed: {str(e)}", flush=True)
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_image_workflow())


