"""
å›¾ç‰‡ç”Ÿæˆå·¥ä½œæµ - åŸºäºæƒ…æ™¯ç”Ÿæˆå›¾ç‰‡
"""
import asyncio
import sys
import os
from typing import Dict, Any, List, Optional
from typing_extensions import TypedDict
from pathlib import Path

# Add project root to Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI

from config.manager import settings
# å·²ç§»é™¤workflow_loggerï¼Œä½¿ç”¨ç®€å•çš„æ—¥å¿—ä¿å­˜
from src.workflow.tools.image_generation_tool import generate_one_img, create_image_generation_tool
from src.workflow.tools.scenario_table_tools import scenario_manager
from src.prompts.image_generation_prompts import IMAGE_SYSTEM_PROMPT, IMAGE_USER_PROMPT_TEMPLATE

# æ¨¡å—çº§åˆå§‹åŒ–scenario_manager
scenario_manager.init(settings.scenario.file_path)


class ImageGenerationState(TypedDict):
    """å›¾ç‰‡ç”Ÿæˆå·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    # è¾“å…¥
    current_scenario: str
    
    # ä¸­é—´çŠ¶æ€
    image_prompt: str
    tool_calls: List[Dict[str, Any]]
    
    # è¾“å‡º
    generated_image_paths: List[str]


async def init_scenario_node(state: ImageGenerationState) -> Dict[str, Any]:
    """åˆå§‹åŒ–èŠ‚ç‚¹ï¼šè¯»å–è¡¨æ ¼å†…å®¹"""
    print("ğŸš€ Scenario table initialization...", flush=True)
    
    import time
    start_time = time.time()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºæ—¥å¿—
    inputs = {
        "node_type": "init_scenario_image",
        "input_state_keys": list(state.keys()) if state else []
    }
    
    try:
        # ç›´æ¥ä»scenario_managerè¯»å–æ‰€æœ‰è¡¨æ ¼å†…å®¹
        current_scenario = scenario_manager.get_all_pretty_tables(description=True, operation_guide=False)
        
        duration = time.time() - start_time
        
        outputs = {
            "scenario_length": len(current_scenario),
            "scenario_preview": current_scenario[:200] + "..." if len(current_scenario) > 200 else current_scenario
        }
        
        # init_scenario_nodeä¸è°ƒç”¨LLMï¼Œæ— éœ€è®°å½•æ—¥å¿—
        
        print(f"  âœ“ Loaded scenario tables, total length: {len(current_scenario)}", flush=True)
        return {"current_scenario": current_scenario}
        
    except Exception as e:
        duration = time.time() - start_time
        import traceback
        error_details = traceback.format_exc()
        
        # é”™è¯¯ä¸è®°å½•æ—¥å¿—ï¼Œåªè¾“å‡ºåˆ°æ§åˆ¶å°
        
        print(f"  âŒ Scenario table initialization failed: {str(e)}", flush=True)
        return {"current_scenario": "Scenario tables not initialized or empty."}


async def llm_generate_prompt_node(state: ImageGenerationState) -> Dict[str, Any]:
    """LLMèŠ‚ç‚¹ï¼šæ ¹æ®æƒ…æ™¯ç”Ÿæˆå›¾ç‰‡æç¤ºè¯å¹¶è°ƒç”¨å·¥å…·"""
    print("ğŸ¨ Generating image prompt from scenario...", flush=True)
    
    import time
    start_time = time.time()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºæ—¥å¿—
    current_scenario = state.get("current_scenario", "")
    
    inputs = {
        "current_scenario_length": len(current_scenario),
        "num_images_requested": settings.comfyui.num_images,
        "model_temperature": 0.7,
        "current_scenario": current_scenario if current_scenario else "[Empty]"
    }
    
    try:
        # è·å–å›¾ç‰‡ç”Ÿæˆå·¥å…·é…ç½®
        image_tool = create_image_generation_tool()
        tool_schema = image_tool["schema"]
        
        # åˆå§‹åŒ–æ¨¡å‹
        agent_config = settings.agent
        extra_body = {"provider": {"only": [agent_config.provider]}} if agent_config.provider else {}
        model = ChatOpenAI(
            model=agent_config.model,
            api_key=agent_config.api_key,
            base_url=agent_config.base_url,
            temperature=0.7,
            extra_body=extra_body
        )
        
        # ä½¿ç”¨æ–°çš„æç¤ºè¯æ¨¡æ¿
        system_prompt = IMAGE_SYSTEM_PROMPT
        user_input = IMAGE_USER_PROMPT_TEMPLATE.format(
            current_scenario=current_scenario,
            num_images=settings.comfyui.num_images
        )
        
        print(f"  Scenario length: {len(current_scenario)}", flush=True)
        print(f"  Generating {settings.comfyui.num_images} images", flush=True)
        
        # è°ƒç”¨LLMç”Ÿæˆå›¾ç‰‡æç¤ºè¯
        response = await model.ainvoke([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ])
        
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        # è§£æLLMè¿”å›çš„å¤šä¸ªå·¥å…·è°ƒç”¨ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        import re
        
        # å°è¯•å¤šç§åŒ¹é…æ¨¡å¼
        patterns = [
            r'generate_one_img\("([^"]+)"\)',  # generate_one_img("prompt")
            r'generate_image\("([^"]+)"\)',    # generate_image("prompt")
            r'"positive_prompt":\s*"([^"]+)"', # JSONæ ¼å¼ä¸­çš„positive_prompt
            r'positive_prompt:\s*([^,\n]+)',   # æ— å¼•å·æ ¼å¼
        ]
        
        matches = []
        for pattern in patterns:
            matches = re.findall(pattern, response_text)
            if matches:
                print(f"  ä½¿ç”¨åŒ¹é…æ¨¡å¼: {pattern}", flush=True)
                break
        
        # å¦‚æœéƒ½æ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯•æŒ‰è¡Œåˆ†å‰²
        if not matches:
            lines = [line.strip() for line in response_text.split('\n') if line.strip()]
            if len(lines) >= settings.comfyui.num_images:
                matches = lines[:settings.comfyui.num_images]
                print(f"  ä½¿ç”¨è¡Œåˆ†å‰²æ¨¡å¼ï¼ŒåŒ¹é…åˆ° {len(matches)} è¡Œ", flush=True)
        
        tool_calls = []
        image_prompts = []
        
        # ä½¿ç”¨è§£æå‡ºçš„æç¤ºè¯åˆ›å»ºå·¥å…·è°ƒç”¨
        if matches:
            for i, prompt in enumerate(matches[:settings.comfyui.num_images]):
                tool_calls.append({
                    "tool_name": "generate_image",
                    "arguments": {
                        "positive_prompt": prompt
                    }
                })
                image_prompts.append(prompt)
                print(f"  æå–æç¤ºè¯ {i+1}: {prompt[:50]}..." if len(prompt) > 50 else f"  æå–æç¤ºè¯ {i+1}: {prompt}", flush=True)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°è¶³å¤Ÿçš„æç¤ºè¯ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬ä½œä¸ºåå¤‡
        while len(tool_calls) < settings.comfyui.num_images:
            fallback_prompt = response_text.strip()
            tool_calls.append({
                "tool_name": "generate_image",
                "arguments": {
                    "positive_prompt": fallback_prompt
                }
            })
            image_prompts.append(fallback_prompt)
            print(f"  ä½¿ç”¨åå¤‡æç¤ºè¯ {len(tool_calls)}: {fallback_prompt[:50]}..." if len(fallback_prompt) > 50 else f"  ä½¿ç”¨åå¤‡æç¤ºè¯ {len(tool_calls)}: {fallback_prompt}", flush=True)
        
        image_prompt = "; ".join(image_prompts[:3])  # åˆå¹¶å‰3ä¸ªä½œä¸ºä¸»è¦æç¤ºè¯
        
        # ä¿å­˜æ—¥å¿—
        from utils.simple_logger import save_log
        from datetime import datetime
        
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "node_type": "llm_image_generation",
            "model_config": {
                "model": agent_config.model,
                "base_url": agent_config.base_url,
                "temperature": 0.7,
                "provider": agent_config.provider if agent_config.provider else None
            },
            "model_input": {
                "system": system_prompt,
                "user": user_input
            },
            "model_output": response_text
        }
        
        log_file = f"./logs/workflow/{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}_image.json"
        save_log(log_file, log_data)
        
        print(f"  âœ“ Generated prompt: {image_prompt[:100]}..." if image_prompt else "  âŒ No prompt generated", flush=True)
        print(f"  âœ“ Tool calls: {len(tool_calls)}", flush=True)
        
        return {
            "image_prompt": image_prompt,
            "tool_calls": tool_calls
        }
        
    except Exception as e:
        duration = time.time() - start_time
        import traceback
        error_details = traceback.format_exc()
        
        # é”™è¯¯ä¸è®°å½•æ—¥å¿—ï¼Œåªè¾“å‡ºåˆ°æ§åˆ¶å°
        
        print(f"âŒ LLM prompt generation failed: {str(e)}", flush=True)
        return {
            "image_prompt": "",
            "tool_calls": []
        }


async def tool_execution_node(state: ImageGenerationState) -> Dict[str, Any]:
    """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ï¼šå¹¶è¡Œæ‰§è¡Œå›¾ç‰‡ç”Ÿæˆå·¥å…·"""
    print("ğŸ› ï¸ Executing image generation tools...", flush=True)
    
    import time
    start_time = time.time()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºæ—¥å¿—
    tool_calls = state.get("tool_calls", [])
    
    inputs = {
        "tool_calls_count": len(tool_calls),
        "tool_calls_details": [
            {
                "tool_name": tc.get("name", ""),
                "args_keys": list(tc.get("args", {}).keys()),
                "positive_prompt_length": len(tc.get("args", {}).get("positive_prompt", ""))
            } for tc in tool_calls
        ]
    }
    
    try:
        if not tool_calls:
            duration = time.time() - start_time
            
            outputs = {
                "generated_paths_count": 0,
                "execution_results": [],
                "error_message": "No tool calls to execute"
            }
            
            # tool_execution_nodeä¸è°ƒç”¨LLMï¼Œæ— éœ€è®°å½•æ—¥å¿—
            
            print("  âŒ No tool calls to execute", flush=True)
            return {"generated_image_paths": []}
        
        print(f"  Executing {len(tool_calls)} tool calls", flush=True)
        
        # è·å–å›¾ç‰‡ç”Ÿæˆå·¥å…·
        image_tool = create_image_generation_tool()
        generate_func = image_tool["function"]
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        generated_paths = []
        execution_details = []
        
        # ä½¿ç”¨ThreadPoolExecutoræ‰§è¡ŒåŒæ­¥å›¾ç‰‡ç”Ÿæˆï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
        import concurrent.futures
        
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = []
            
            for i, tool_call in enumerate(tool_calls):
                tool_name = tool_call.get("tool_name", "")
                args = tool_call.get("arguments", {})
                
                if tool_name == "generate_image":
                    positive_prompt = args.get("positive_prompt", "")
                    print(f"  [{i+1}] Generating image with prompt: {positive_prompt[:50]}...", flush=True)
                    
                    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥å›¾ç‰‡ç”Ÿæˆ
                    future = executor.submit(generate_one_img, positive_prompt)
                    futures.append((future, i, tool_call))
                else:
                    print(f"  [{i+1}] âŒ Unknown tool: {tool_name}", flush=True)
                    execution_details.append({
                        "index": i,
                        "tool_name": tool_name,
                        "status": "error",
                        "error": f"Unknown tool: {tool_name}",
                        "args": args
                    })
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future, i, tool_call in futures:
                tool_name = tool_call.get("tool_name", "")
                args = tool_call.get("arguments", {})
                positive_prompt = args.get("positive_prompt", "")
                
                try:
                    # åœ¨äº‹ä»¶å¾ªç¯ä¸­ç­‰å¾…çº¿ç¨‹æ± ç»“æœ
                    result = await asyncio.get_event_loop().run_in_executor(None, future.result)
                    print(f"  [{i+1}] âœ“ Image generated: {result}", flush=True)
                    
                    # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºé”™è¯¯
                    if result and not result.startswith("é”™è¯¯"):
                        generated_paths.append(result)
                        execution_details.append({
                            "index": i,
                            "tool_name": tool_name,
                            "status": "success",
                            "result": result,
                            "args": args,
                            "positive_prompt": positive_prompt[:100] + "..." if len(positive_prompt) > 100 else positive_prompt
                        })
                    else:
                        generated_paths.append(result)
                        execution_details.append({
                            "index": i,
                            "tool_name": tool_name,
                            "status": "error",
                            "error": result,
                            "args": args,
                            "positive_prompt": positive_prompt[:100] + "..." if len(positive_prompt) > 100 else positive_prompt
                        })
                        
                except Exception as e:
                    error_msg = f"é”™è¯¯ï¼š{str(e)}"
                    generated_paths.append(error_msg)
                    print(f"  [{i+1}] âŒ Generation failed: {str(e)}", flush=True)
                    
                    execution_details.append({
                        "index": i,
                        "tool_name": tool_name,
                        "status": "error",
                        "error": str(e),
                        "args": args,
                        "positive_prompt": positive_prompt[:100] + "..." if len(positive_prompt) > 100 else positive_prompt
                    })
        
        duration = time.time() - start_time
        
        outputs = {
            "generated_paths_count": len(generated_paths),
            "generated_paths": generated_paths,
            "execution_details": execution_details,
            "successful_generations": len([d for d in execution_details if d.get("status") == "success"]),
            "failed_generations": len([d for d in execution_details if d.get("status") == "error"])
        }
        
        # tool_execution_nodeä¸è°ƒç”¨LLMï¼Œæ— éœ€è®°å½•æ—¥å¿—
        
        print(f"  âœ“ Total generated: {len(generated_paths)} images", flush=True)
        return {"generated_image_paths": generated_paths}
            
    except Exception as e:
        duration = time.time() - start_time
        import traceback
        error_details = traceback.format_exc()
        
        outputs = {
            "generated_paths_count": 0,
            "execution_results": [],
            "error_message": str(e)
        }
        
        # é”™è¯¯ä¸è®°å½•æ—¥å¿—ï¼Œåªè¾“å‡ºåˆ°æ§åˆ¶å°
        
        print(f"âŒ Tool execution failed: {str(e)}", flush=True)
        return {"generated_image_paths": [f"é”™è¯¯ï¼š{str(e)}"]}


def create_image_generation_workflow():
    """åˆ›å»ºå›¾ç‰‡ç”Ÿæˆå·¥ä½œæµ"""
    builder = StateGraph(ImageGenerationState)
    
    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("init_scenario", init_scenario_node)
    builder.add_node("llm_generate_prompt", llm_generate_prompt_node)
    builder.add_node("tool_execution", tool_execution_node)
    
    # æ·»åŠ è¾¹ï¼šçº¿æ€§æµç¨‹
    builder.add_edge(START, "init_scenario")
    builder.add_edge("init_scenario", "llm_generate_prompt")
    builder.add_edge("llm_generate_prompt", "tool_execution")
    builder.add_edge("tool_execution", END)
    
    return builder.compile()


async def test_image_workflow():
    """æµ‹è¯•å›¾ç‰‡ç”Ÿæˆå·¥ä½œæµ"""
    print("ğŸ§ª Image generation workflow test starting...", flush=True)
    
    workflow = create_image_generation_workflow()
    
    test_input = {
        "current_scenario": ""  # ç©ºåœºæ™¯ï¼Œè®©initèŠ‚ç‚¹ä»è¡¨æ ¼åŠ è½½
    }
    
    try:
        result = await workflow.ainvoke(test_input)
        print(f"âœ… Image workflow test completed successfully", flush=True)
        print(f"Generated paths: {result.get('generated_image_paths', [])}", flush=True)
        
    except Exception as e:
        print(f"âŒ Image workflow test failed: {str(e)}", flush=True)
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_image_workflow())


