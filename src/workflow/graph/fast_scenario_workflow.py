"""
Fast Scenario Update Workflow - å››èŠ‚ç‚¹å¿«é€Ÿç‰ˆæœ¬
ä½¿ç”¨ä¸¤ä¸ªLLMèŠ‚ç‚¹å’Œä¸¤ä¸ªå·¥å…·é›†åˆèŠ‚ç‚¹å®ç°å¿«é€Ÿæƒ…æ™¯æ›´æ–°
"""
import asyncio
import sys
import os
from typing import Dict, Any, List, Optional
from typing_extensions import TypedDict

# Add project root to Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

from config.manager import settings
# å·²ç§»é™¤workflow_loggerï¼Œä½¿ç”¨ç®€å•çš„æ—¥å¿—ä¿å­˜
from src.workflow.tools.re_search_tool import re_search
from src.workflow.tools.scenario_table_tools import scenario_manager, create_row, delete_row, update_cell

# æ¨¡å—çº§åˆå§‹åŒ–scenario_manager
scenario_manager.init(settings.scenario.file_path)


class FastState(TypedDict):
    """å¿«é€Ÿå·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    # è¾“å…¥å‚æ•°
    current_scenario: str
    messages: List[Dict[str, Any]]
    
    # ä¸­é—´çŠ¶æ€
    last_ai_message: str
    search_tool_calls: List[Dict[str, Any]]
    search_results: List[str]
    edit_tool_calls: List[Dict[str, Any]]
    
    # è¾“å‡ºç»“æœ
    final_scenario: str
    
    # è¯·æ±‚å¤„ç†ç›¸å…³
    request_id: Optional[str] = None
    original_messages: Optional[List[Dict]] = None
    api_key: Optional[str] = None
    model: Optional[str] = None
    stream: Optional[bool] = None


def create_wikipedia_tool():
    """åˆ›å»ºWikipediaæœç´¢å·¥å…·"""
    api_wrapper = WikipediaAPIWrapper(
        top_k_results=1,
        doc_content_chars_max=2000,
        lang="en"
    )
    
    return WikipediaQueryRun(
        name="wikipedia_search",
        description="æœç´¢Wikipediaä¿¡æ¯ã€‚è¾“å…¥åº”è¯¥æ˜¯æœç´¢æŸ¥è¯¢ã€‚",
        api_wrapper=api_wrapper
    )


def extract_latest_ai_message(messages: List[Dict[str, Any]], offset: int = 1) -> str:
    """æå–æœ€æ–°çš„AIæ¶ˆæ¯"""
    ai_messages = []
    for msg in messages:
        if msg.get("role") == "assistant":
            ai_messages.append(msg.get("content", ""))
    
    if len(ai_messages) >= offset:
        return ai_messages[-offset]
    elif ai_messages:
        return ai_messages[-1]
    else:
        return ""




async def llm_search_node(state: FastState) -> Dict[str, Any]:
    """ç¬¬ä¸€ä¸ªLLMèŠ‚ç‚¹ï¼šåˆ¤æ–­éœ€è¦æœç´¢çš„å†…å®¹å¹¶è¾“å‡ºå·¥å…·è°ƒç”¨"""
    print("ğŸ” Memory search node executing...", flush=True)
    
    import time
    start_time = time.time()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºæ—¥å¿—
    messages = state.get("messages", [])
    current_scenario = state.get("current_scenario", "")
    
    # å¦‚æœé…ç½®ä¸º-1ï¼Œè‡ªåŠ¨æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå†…å®¹é•¿åº¦>100çš„AIæ¶ˆæ¯ç´¢å¼•
    ai_msg_index = settings.langgraph.last_ai_messages_index
    if ai_msg_index == -1:
        from utils.messages_process import auto_find_ai_message_index
        ai_msg_index = auto_find_ai_message_index(messages)
    
    last_ai_message = extract_latest_ai_message(messages, ai_msg_index)
    
    inputs = {
        "current_scenario_length": len(current_scenario),
        "messages_count": len(messages),
        "last_ai_message_length": len(last_ai_message),
        "current_scenario": current_scenario if current_scenario else "[Empty]",
        "last_ai_message": last_ai_message if last_ai_message else "[Empty]"
    }
    
    try:
        # å¯¼å…¥æœç´¢LLMæç¤ºè¯å’Œç»“æ„åŒ–å·¥å…·è¾…åŠ©å‡½æ•°
        from src.prompts.fast_memory_search_prompts import SEARCH_LLM_PROMPT, SEARCH_USER_TEMPLATE
        from src.workflow.tools.structured_tool_helper import generate_pydantic_tool_prompts, parse_tool_calls
        
        # åˆå§‹åŒ–æ¨¡å‹
        agent_config = settings.agent
        extra_body = {"provider": {"only": [agent_config.provider]}} if agent_config.provider else {}
        model = ChatOpenAI(
            model=agent_config.model,
            api_key=agent_config.api_key,
            base_url=agent_config.base_url,
            temperature=agent_config.temperature,
            extra_body=extra_body
        )
        
        # å®šä¹‰æœç´¢å·¥å…·ï¼ˆPydanticæ¨¡å‹ï¼‰
        from pydantic import BaseModel, Field
        
        class SearchMemory(BaseModel):
            """æœç´¢å†…éƒ¨è®°å¿†"""
            pattern: str = Field(description="æ­£åˆ™æœç´¢æ¨¡å¼")
        
        class SearchWikipedia(BaseModel):
            """æœç´¢Wikipedia"""
            query: str = Field(description="æœç´¢æŸ¥è¯¢")
        
        # ç”Ÿæˆå·¥å…·æç¤ºè¯
        search_tools = [SearchMemory, SearchWikipedia]
        tools_description_system, tools_description_user = generate_pydantic_tool_prompts(search_tools)
        
        # æ„å»ºæç¤º
        system_prompt = SEARCH_LLM_PROMPT.format(tools_description_system=tools_description_system)
        user_input = SEARCH_USER_TEMPLATE.format(
            current_scenario=current_scenario,
            last_ai_message=last_ai_message,
            tools_description_user=tools_description_user
        )
        
        # ç®€æ´çš„è¾“å…¥ä¿¡æ¯
        print(f"  Messages: {len(messages)}, Scenario length: {len(current_scenario)}", flush=True)
        
        # è°ƒç”¨LLM
        response = await model.ainvoke([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ])
        
        # è§£æç»“æ„åŒ–è¾“å‡º
        response_text = response.content if hasattr(response, 'content') else str(response)
        tool_calls = parse_tool_calls(response_text)
        print(f"  Planned {len(tool_calls)} search operations: {[tc.get('tool_name') for tc in tool_calls]}", flush=True)
        
        # ä¿å­˜æ—¥å¿—
        from utils.simple_logger import save_log
        from datetime import datetime
        
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "node_type": "llm_search_fast",
            "model_config": {
                "model": agent_config.model,
                "base_url": agent_config.base_url,
                "temperature": agent_config.temperature,
                "provider": agent_config.provider if agent_config.provider else None
            },
            "model_input": {
                "system": system_prompt,
                "user": user_input
            },
            "model_output": response_text
        }
        
        log_file = f"./logs/workflow/{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}_search.json"
        save_log(log_file, log_data)
        
        return {
            "last_ai_message": last_ai_message,
            "search_tool_calls": tool_calls
        }
        
    except Exception as e:
        duration = time.time() - start_time
        import traceback
        error_details = traceback.format_exc()
        
        # é”™è¯¯ä¸è®°å½•æ—¥å¿—ï¼Œåªè¾“å‡ºåˆ°æ§åˆ¶å°
        
        print(f"âŒ Memory search node failed: {str(e)}", flush=True)
        return {
            "last_ai_message": "",
            "search_tool_calls": []
        }


async def tool_search_node(state: FastState) -> Dict[str, Any]:
    """ç¬¬ä¸€ä¸ªå·¥å…·èŠ‚ç‚¹ï¼šæ‰§è¡Œæœç´¢ç›¸å…³å·¥å…·"""
    print("ğŸ› ï¸ Search tool node executing...", flush=True)
    
    try:
        tool_calls = state.get("search_tool_calls", [])
        search_results = []
        messages = state.get("messages", [])
        
        # åˆ›å»ºWikipediaå·¥å…·
        wikipedia_tool = create_wikipedia_tool()
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        for tool_call in tool_calls:
            tool_name = tool_call.get("tool_name", "")
            args = tool_call.get("arguments", {})
            
            try:
                if tool_name == "SearchMemory":
                    # è°ƒç”¨re_searchï¼Œéœ€è¦ä¼ é€’config
                    from langchain_core.runnables import RunnableConfig
                    config = RunnableConfig(
                        configurable={
                            "conversation_history": messages
                        }
                    )
                    result = re_search.invoke(args.get("pattern", ""), config)
                    search_results.append(f"[Internal Memory] {result}")
                
                elif tool_name == "SearchWikipedia":
                    # è°ƒç”¨Wikipediaæœç´¢
                    result = wikipedia_tool.invoke(args.get("query", ""))
                    search_results.append(f"[External Knowledge] {result}")
                    
            except Exception as e:
                search_results.append(f"[{tool_name}] Search failed: {str(e)}")
        
        return {"search_results": search_results}
        
    except Exception as e:
        print(f"âŒ Search tool node failed: {str(e)}", flush=True)
        return {"search_results": []}


async def llm_edit_node(state: FastState) -> Dict[str, Any]:
    """ç¬¬äºŒä¸ªLLMèŠ‚ç‚¹ï¼šå†³å®šå¦‚ä½•ç¼–è¾‘æƒ…æ™¯æ–‡ä»¶å¹¶è¾“å‡ºå·¥å…·è°ƒç”¨"""
    print("âœï¸ Scenario updater node executing...", flush=True)
    
    import time
    start_time = time.time()
    
    # å‡†å¤‡è¾“å…¥æ•°æ®ç”¨äºæ—¥å¿—
    current_scenario = state.get("current_scenario", "")
    last_ai_message = state.get("last_ai_message", "")
    search_results = state.get("search_results", [])
    
    inputs = {
        "current_scenario_length": len(current_scenario),
        "last_ai_message_length": len(last_ai_message),
        "search_results_count": len(search_results),
        "search_results_total_length": sum(len(r) for r in search_results),
        "current_scenario": current_scenario if current_scenario else "[Empty]",
        "last_ai_message": last_ai_message if last_ai_message else "[Empty]",
        "search_results": search_results  # å®Œæ•´çš„æœç´¢ç»“æœ
    }
    
    try:
        # å¯¼å…¥ç¼–è¾‘LLMæç¤ºè¯å’Œç»“æ„åŒ–å·¥å…·è¾…åŠ©å‡½æ•°
        from src.prompts.fast_scenario_edit_prompts import EDIT_LLM_PROMPT, EDIT_USER_TEMPLATE
        from src.workflow.tools.structured_tool_helper import generate_tool_prompts, parse_tool_calls
        
        # åˆå§‹åŒ–æ¨¡å‹
        agent_config = settings.agent
        extra_body = {"provider": {"only": [agent_config.provider]}} if agent_config.provider else {}
        model = ChatOpenAI(
            model=agent_config.model,
            api_key=agent_config.api_key,
            base_url=agent_config.base_url,
            temperature=agent_config.temperature,
            extra_body=extra_body
        )
        
        # ç›´æ¥ä½¿ç”¨å·²å®šä¹‰çš„å·¥å…·
        edit_tools = [create_row, delete_row, update_cell]
        tools_description_system, tools_description_user = generate_tool_prompts(edit_tools)
        
        # æ„å»ºæç¤º
        search_results_text = "\n".join(search_results) if search_results else "No search results"
        
        # åŠ¨æ€å¡«å……ç³»ç»Ÿæç¤ºè¯ä¸­çš„schema
        dynamic_system_prompt = EDIT_LLM_PROMPT.format(
            tools_description_system=tools_description_system,
            schema_text=scenario_manager.get_table_schema_text()
        )
        
        user_input = EDIT_USER_TEMPLATE.format(
            tools_description_user=tools_description_user,
            current_scenario=current_scenario,
            last_ai_message=last_ai_message,
            search_results=search_results_text
        )
        
        # ç®€æ´çš„è¾“å…¥ä¿¡æ¯
        print(f"  Search results: {len(search_results)}, Total search text length: {sum(len(r) for r in search_results)}", flush=True)
        
        # è°ƒç”¨LLM
        response = await model.ainvoke([
            {"role": "system", "content": dynamic_system_prompt},
            {"role": "user", "content": user_input}
        ])
        
        # è§£æç»“æ„åŒ–è¾“å‡º
        response_text = response.content if hasattr(response, 'content') else str(response)
        tool_calls = parse_tool_calls(response_text)
        print(f"  Planned {len(tool_calls)} table operations: {[tc.get('tool_name') for tc in tool_calls]}", flush=True)
        
        # ä¿å­˜æ—¥å¿—
        from utils.simple_logger import save_log
        from datetime import datetime
        
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "node_type": "llm_edit_fast",
            "model_config": {
                "model": agent_config.model,
                "base_url": agent_config.base_url,
                "temperature": agent_config.temperature,
                "provider": agent_config.provider if agent_config.provider else None
            },
            "model_input": {
                "system": dynamic_system_prompt,
                "user": user_input
            },
            "model_output": response_text
        }
        
        log_file = f"./logs/workflow/{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}_edit.json"
        save_log(log_file, log_data)
        
        return {"edit_tool_calls": tool_calls}
        
    except Exception as e:
        duration = time.time() - start_time
        import traceback
        error_details = traceback.format_exc()
        
        # é”™è¯¯ä¸è®°å½•æ—¥å¿—ï¼Œåªè¾“å‡ºåˆ°æ§åˆ¶å°
        
        print(f"âŒ Scenario updater node failed: {str(e)}", flush=True)
        return {"edit_tool_calls": []}


async def tool_edit_node(state: FastState) -> Dict[str, Any]:
    """ç¬¬äºŒä¸ªå·¥å…·èŠ‚ç‚¹ï¼šæ‰§è¡Œè¡¨æ ¼ç¼–è¾‘æ“ä½œ"""
    print("ğŸ“ Table edit tool node executing...", flush=True)
    
    try:
        tool_calls = state.get("edit_tool_calls", [])
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        for tool_call in tool_calls:
            tool_name = tool_call.get("tool_name", "")
            args = tool_call.get("arguments", {})
            
            try:
                if tool_name == "create_row":
                    result = create_row.invoke({
                        "table_name": args.get("table_name", ""),
                        "row_data": args.get("row_data", {})
                    })
                    print(f"  âœ“ Created row in {args.get('table_name', 'unknown table')}", flush=True)
                
                elif tool_name == "delete_row":
                    result = delete_row.invoke({
                        "table_name": args.get("table_name", ""),
                        "row_id": args.get("row_id", "")
                    })
                    print(f"  âœ“ Deleted row {args.get('row_id', '')} from {args.get('table_name', 'unknown table')}", flush=True)
                
                elif tool_name == "update_cell":
                    result = update_cell.invoke({
                        "table_name": args.get("table_name", ""),
                        "row_id": args.get("row_id", ""),
                        "column_name": args.get("column_name", ""),
                        "new_value": args.get("new_value", "")
                    })
                    print(f"  âœ“ Updated {args.get('column_name', '')} in {args.get('table_name', 'unknown table')}", flush=True)
                    
            except Exception as e:
                print(f"  âŒ Tool execution failed {tool_name}: {str(e)}", flush=True)
        
        return {}  # ä¸éœ€è¦è¿”å›final_scenarioï¼Œæ³¨å…¥æ—¶ä¼šé‡æ–°è¯»å–
        
    except Exception as e:
        print(f"âŒ Table edit tool node failed: {str(e)}", flush=True)
        return {}


async def init_scenario_node(state: FastState) -> Dict[str, Any]:
    """åˆå§‹åŒ–èŠ‚ç‚¹ï¼šè¯»å–è¡¨æ ¼å†…å®¹"""
    print("ğŸš€ Scenario table initialization...", flush=True)
    
    try:
        # ç›´æ¥ä»scenario_managerè¯»å–æ‰€æœ‰è¡¨æ ¼å†…å®¹
        current_scenario = scenario_manager.get_all_pretty_tables(description=True, operation_guide=True)
        
        print(f"  âœ“ Loaded scenario tables, total length: {len(current_scenario)}", flush=True)
        return {"current_scenario": current_scenario}
        
    except Exception as e:
        print(f"  âŒ Scenario table initialization failed: {str(e)}", flush=True)
        return {"current_scenario": "Scenario tables not initialized or empty."}


def create_fast_scenario_workflow():
    """åˆ›å»ºå¿«é€Ÿæƒ…æ™¯æ›´æ–°å·¥ä½œæµ"""
    builder = StateGraph(FastState)
    
    # æ·»åŠ äº”ä¸ªèŠ‚ç‚¹ï¼ˆåŒ…å«åˆå§‹åŒ–èŠ‚ç‚¹ï¼‰
    builder.add_node("init_scenario", init_scenario_node)
    builder.add_node("llm_search", llm_search_node)
    builder.add_node("tool_search", tool_search_node)
    builder.add_node("llm_edit", llm_edit_node)
    builder.add_node("tool_edit", tool_edit_node)
    
    # æ·»åŠ è¾¹ï¼šçº¿æ€§æµç¨‹
    builder.add_edge(START, "init_scenario")
    builder.add_edge("init_scenario", "llm_search")
    builder.add_edge("llm_search", "tool_search")
    builder.add_edge("tool_search", "llm_edit")
    builder.add_edge("llm_edit", "tool_edit")
    builder.add_edge("tool_edit", END)
    
    return builder.compile()
