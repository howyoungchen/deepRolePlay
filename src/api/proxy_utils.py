"""
ä»£ç†æœåŠ¡çš„ç»Ÿä¸€å·¥å…·ç±»é›†åˆ
åŒ…å«è®¤è¯ã€å“åº”æ„å»ºã€æµå¼å¤„ç†ã€å·¥ä½œæµè¾…åŠ©ã€æ—¥å¿—è®°å½•å’Œç›®å½•æ“ä½œç­‰åŠŸèƒ½
"""
import json
import time
import uuid
import base64
import os
import glob
from typing import Dict, Any, Optional, List, Callable, AsyncGenerator
from fastapi import Request, Response
from fastapi.responses import StreamingResponse, JSONResponse
from config.manager import settings
import difflib


class AuthUtils:
    """APIå¯†é’¥å’Œè®¤è¯å·¥å…·ç±»"""
    
    @staticmethod
    def extract_api_key(request: Request) -> str:
        """ä»é…ç½®æ–‡ä»¶ä¸­æå–APIå¯†é’¥"""
        return settings.proxy.api_key or ""
    
    @staticmethod
    def get_request_headers(request: Request) -> dict:
        """è·å–è½¬å‘è¯·æ±‚æ‰€éœ€çš„å¤´éƒ¨ä¿¡æ¯"""
        headers = {
            "Content-Type": "application/json",
            "User-Agent": "DeepRolePlay-Proxy/1.0"
        }
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„APIå¯†é’¥
        api_key = AuthUtils.extract_api_key(request)
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"
        
        return headers


class ResponseBuilder:
    """ç»Ÿä¸€çš„OpenAIå…¼å®¹å“åº”æ„å»ºå™¨"""
    
    @staticmethod
    def create_chat_completion_response(
        request_id: str,
        model: str,
        content: str,
        stream: bool = False,
        finish_reason: str = "stop",
        usage_tokens: Optional[Dict[str, int]] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºèŠå¤©å®Œæˆå“åº”"""
        base_response = {
            "id": f"chatcmpl-{request_id}",
            "created": int(time.time()),
            "model": model,
        }
        
        if stream:
            base_response.update({
                "object": "chat.completion.chunk",
                "choices": [{
                    "index": 0,
                    "delta": {
                        "role": "assistant",
                        "content": content
                    },
                    "finish_reason": finish_reason
                }]
            })
        else:
            base_response.update({
                "object": "chat.completion",
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": content
                    },
                    "finish_reason": finish_reason
                }],
                "usage": usage_tokens or {
                    "prompt_tokens": 10,
                    "completion_tokens": len(content),
                    "total_tokens": 10 + len(content)
                }
            })
        
        return base_response
    
    @staticmethod
    def create_special_response(response_type: str, request_id: str, model: str, stream: bool = False) -> Dict[str, Any]:
        """åˆ›å»ºç‰¹æ®Šå“åº”ï¼ˆè°ƒè¯•æ¨¡å¼ã€æ–°å¯¹è¯ç­‰ï¼‰"""
        if response_type == "debug":
            try:
                with open("/home/chiye/worklab/deepRolePlay/pics/generate.png", "rb") as img_file:
                    img_data = base64.b64encode(img_file.read()).decode('utf-8')
                    content = (f'images display:\n'
                              f'<img src="data:image/png;base64,{img_data}" alt="Wizard 1" style="max-width: 300px;">'
                    )
            except FileNotFoundError:
                content = "IMG ERROR"
        elif response_type == "backend_command":
            content = "Backend command executed."
        else:
            content = f"Special response: {response_type}"
        
        return ResponseBuilder.create_chat_completion_response(
            request_id=request_id,
            model=model,
            content=content,
            stream=stream
        )
    
    @staticmethod
    def create_error_response(
        error_message: str,
        error_type: str = "server_error",
        error_code: str = "INTERNAL_ERROR",
        status_code: int = 500
    ) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return {
            "error": {
                "message": error_message,
                "type": error_type,
                "code": error_code
            }
        }


class StreamingHandler:
    """ç»Ÿä¸€çš„æµå¼å“åº”å¤„ç†å™¨"""
    
    @staticmethod
    async def create_simple_streaming_response(
        request: Request,
        response_data: Dict[str, Any],
        request_id: Optional[str] = None,
        extra_headers: Optional[Dict[str, str]] = None,
        log_data: Optional[Dict[str, Any]] = None
    ) -> StreamingResponse:
        """åˆ›å»ºç®€å•çš„æµå¼å“åº”ï¼ˆç”¨äºè°ƒè¯•æ¨¡å¼å’Œæ–°å¯¹è¯ï¼‰"""
        if not request_id:
            request_id = str(uuid.uuid4())
        
        async def stream_generator():
            yield f"data: {json.dumps(response_data)}\n\n"
            yield "data: [DONE]\n\n"
        
        headers = {
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Request-ID": request_id
        }
        
        if extra_headers:
            headers.update(extra_headers)
        
        response = StreamingResponse(
            stream_generator(),
            media_type="text/event-stream",
            headers=headers
        )
        
        # è®°å½•æ—¥å¿—
        if log_data:
            await LoggingUtils.log_response(
                request=request,
                response=response,
                request_body=log_data.get("request_body", {}),
                response_body=log_data.get("response_body", {}),
                duration=log_data.get("duration", 0.001),
                request_id=request_id
            )
        
        return response
    
    @staticmethod
    def create_workflow_streaming_response(
        request: Request,
        workflow_generator: Callable,
        request_id: Optional[str] = None
    ) -> StreamingResponse:
        """åˆ›å»ºå·¥ä½œæµçš„æµå¼å“åº”"""
        if not request_id:
            request_id = str(uuid.uuid4())
        
        return StreamingResponse(
            workflow_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Request-ID": request_id
            }
        )


class WorkflowHelper:
    """å·¥ä½œæµç›¸å…³çš„è¾…åŠ©å·¥å…·ç±»"""
    
    @staticmethod
    def prepare_workflow_input(
        request,
        chat_request,
        request_id: str = None,
        current_scenario: str = ""
    ) -> Dict[str, Any]:
        """å‡†å¤‡å·¥ä½œæµè¾“å…¥æ•°æ®"""
        if not request_id:
            request_id = str(uuid.uuid4())
        
        api_key = AuthUtils.extract_api_key(request)
        
        return {
            "request_id": request_id,
            "original_messages": [msg.model_dump() for msg in chat_request.messages],
            "messages": [msg.model_dump() for msg in chat_request.messages],
            "current_scenario": current_scenario,
            "api_key": api_key,
            "model": chat_request.model,
            "stream": chat_request.stream,
            "chat_request": chat_request  # æ·»åŠ å®Œæ•´çš„è¯·æ±‚å¯¹è±¡ä»¥æ”¯æŒæ‰€æœ‰å‚æ•°è½¬å‘
        }
    

    @staticmethod
    def get_recent_user_messages_content(messages: List, count: int) -> List[str]:
        """ä»å€’æ•°countæ¡ç”¨æˆ·æ¶ˆæ¯ä¸­è·å–å†…å®¹åˆ—è¡¨"""
        recent_user_messages = []
        for message in reversed(messages):
            if hasattr(message, 'role') and message.role == "user":
                recent_user_messages.append(message)
                if len(recent_user_messages) >= count:
                    break
        
        # è¿”å›è¿™äº›æ¶ˆæ¯çš„content
        contents = []
        for msg in recent_user_messages:
            content = msg.content if hasattr(msg, 'content') else ""
            contents.append(content)
        return contents
    
    @staticmethod
    def calculate_message_similarity(msg1: str, msg2: str, threshold: float = 0.9) -> tuple[bool, float]:
        """
        è®¡ç®—ä¸¤æ¡æ¶ˆæ¯çš„ç›¸ä¼¼åº¦
        
        Args:
            msg1: ç¬¬ä¸€æ¡æ¶ˆæ¯
            msg2: ç¬¬äºŒæ¡æ¶ˆæ¯
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.9ï¼ˆ90%ï¼‰
            
        Returns:
            tuple: (æ˜¯å¦ç›¸ä¼¼ï¼ˆTrueè¡¨ç¤ºç›¸ä¼¼ï¼ŒFalseè¡¨ç¤ºå·®å¼‚è¾ƒå¤§ï¼‰, å®é™…ç›¸ä¼¼åº¦å€¼)
        """
        if not msg1 and not msg2:
            return True, 1.0
            
        if not msg1 or not msg2:
            return False, 0.0
        
        # ä½¿ç”¨difflibè®¡ç®—åºåˆ—ç›¸ä¼¼åº¦
        similarity = difflib.SequenceMatcher(None, msg1, msg2).ratio()
        
        return similarity >= threshold, similarity
    
    @staticmethod
    def handle_scenario_clear_strategy(messages: List, message_cache: List[str] = None) -> tuple[bool, List[str]]:
        """
        æ ¹æ®é…ç½®çš„ç­–ç•¥å¤„ç†æƒ…æ™¯æ–‡ä»¶æ¸…ç†
        
        Args:
            messages: å½“å‰è¯·æ±‚çš„æ¶ˆæ¯åˆ—è¡¨
            message_cache: å½“å‰ç¼“å­˜çš„æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            tuple: (æ˜¯å¦æ‰§è¡Œäº†æ¸…ç†, æ–°çš„æ¶ˆæ¯ç¼“å­˜)
        """
        from config.manager import settings
        
        strategy = settings.scenario.clear_strategy
        
        # manual ç­–ç•¥ï¼šè·³è¿‡æ¸…ç†
        if strategy == "manual":
            return False, message_cache or []
        
        # always ç­–ç•¥ï¼šæ€»æ˜¯æ¸…ç†
        if strategy == "always":
            WorkflowHelper._clear_scenario_file()
            return True, []
        
        # auto ç­–ç•¥ï¼šæ™ºèƒ½åˆ¤æ–­ï¼ˆåªå¯¹æ¯”ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼‰
        if strategy == "auto":
            # è·å–ç¬¬ä¸€æ¡æ¶ˆæ¯çš„å†…å®¹
            current_first_message = ""
            if messages:
                first_msg = messages[0]
                current_first_message = first_msg.get("content", "") if hasattr(first_msg, 'get') else getattr(first_msg, 'content', "")
            
            # è·å–ç¼“å­˜çš„ç¬¬ä¸€æ¡æ¶ˆæ¯
            cached_first_message = message_cache[0] if message_cache else ""
            
            # ä½¿ç”¨é…ç½®çš„ç›¸ä¼¼åº¦é˜ˆå€¼è¿›è¡Œå¯¹æ¯”
            threshold = settings.scenario.similarity_threshold
            is_similar, similarity_score = WorkflowHelper.calculate_message_similarity(
                cached_first_message, current_first_message, threshold
            )
            
            # å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–ä¸å½“å‰ç¬¬ä¸€æ¡æ¶ˆæ¯ç›¸ä¼¼åº¦ä¸å¤Ÿï¼Œåˆ™æ¸…ç†å¹¶æ›´æ–°ç¼“å­˜
            if not message_cache or not is_similar:
                if message_cache:  # åªæœ‰å­˜åœ¨ç¼“å­˜æ—¶æ‰æ‰“å°ç›¸ä¼¼åº¦ä¿¡æ¯
                    print(f"[æ¶ˆæ¯ç¼“å­˜] æ£€æµ‹åˆ°æ–°å¯¹è¯ï¼Œç›¸ä¼¼åº¦: {similarity_score:.3f} < {threshold:.1f}, æ¸…ç†scenarioæ–‡ä»¶")
                WorkflowHelper._clear_scenario_file()
                return True, [current_first_message]
            
            # ç¼“å­˜ç›¸ä¼¼ï¼Œè·³è¿‡æ¸…ç†
            return False, message_cache
        
        # æœªçŸ¥ç­–ç•¥ï¼Œé»˜è®¤ä¸æ¸…ç†
        return False, message_cache or []
    
    @staticmethod
    def _clear_scenario_file():
        """æ¸…ç†å•ä¸ªæƒ…æ™¯æ–‡ä»¶"""
        from config.manager import settings
        import os
        
        scenario_file_path = settings.scenario.file_path
        
        try:
            if os.path.exists(scenario_file_path):
                os.remove(scenario_file_path)
                print(f"Scenario file cleared: {scenario_file_path}")
        except Exception as e:
            print(f"Failed to clear scenario file: {e}")



class LoggingUtils:
    """æ—¥å¿—è®°å½•å·¥å…·ç±»"""
    
    @staticmethod
    async def log_response(
        request: Request,
        response: Optional[Response],
        request_body: Dict[str, Any],
        response_body: Dict[str, Any],
        duration: float,
        request_id: str
    ):
        """è®°å½•è¯·æ±‚å“åº”æ—¥å¿—"""
        pass
    
    @staticmethod
    async def save_full_messages(chat_request: Any, request_id: str):
        """ä¿å­˜å®Œæ•´çš„è¯·æ±‚å‚æ•°"""
        if not settings.log.save_request_origin_messages:
            return
        
        import json
        from datetime import datetime
        from pathlib import Path
        
        try:
            # ä½¿ç”¨ model_dump() è·å–æ‰€æœ‰è¯·æ±‚å‚æ•°
            request_data = chat_request.model_dump()
            
            # åˆ›å»ºæŒ‰æ—¶é—´æˆ³å‘½åçš„ä¼šè¯æ—¥å¿—ç›®å½•
            timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
            session_log_dir = Path(settings.log.get_session_log_path(timestamp))
            session_log_dir.mkdir(parents=True, exist_ok=True)
            
            # æ„å»ºå®Œæ•´çš„æ—¥å¿—æ•°æ®
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "request_id": request_id,
                **request_data  # å±•å¼€æ‰€æœ‰è¯·æ±‚å‚æ•°
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            filename = f"request_messages_{request_id[:8]}.json"
            log_path = session_log_dir / filename
            
            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
                
            print(f"\\ Full request saved: {log_path}")
            
        except Exception as e:
            print(f"âŒ Failed to save full request: {e}")




class BackendCommandHandler:
    """DRPåå°å‘½ä»¤å¤„ç†å™¨"""
    
    @staticmethod
    def parse_command_from_messages(messages: List, count: int = 10) -> Optional[str]:
        """ä»æœ€è¿‘çš„æ¶ˆæ¯ä¸­è§£æåå°å‘½ä»¤ï¼ˆå€’åºæ£€æŸ¥ï¼‰"""
        if not messages:
            return None
        
        # ä»åå¾€å‰æ£€æŸ¥æœ€å¤šcountæ¡æ¶ˆæ¯
        check_count = min(count, len(messages))
        
        # å€’åºé€æ¡æ£€æŸ¥
        for i in range(1, check_count + 1):
            message = messages[-i]  # ä»æœ€åä¸€æ¡å¼€å§‹
            
            # è·å–æ¶ˆæ¯å†…å®¹
            if hasattr(message, 'content'):
                content = message.content
            elif isinstance(message, dict) and 'content' in message:
                content = message['content']
            else:
                continue
            
            # è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ£€æŸ¥
            content_lower = content.lower()
            
            # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥å‘½ä»¤ï¼ˆæ‰¾åˆ°è°å°±å…ˆè¿”å›è°ï¼‰
            if '$reset' in content_lower:
                return 'reset'
            elif '$rm' in content_lower:
                return 'rm'
            elif '$show' in content_lower:
                return 'show'
            elif '$fast' in content_lower:
                return 'workflow_switch_fast'
            elif '$drp' in content_lower:
                return 'workflow_switch_drp'
            elif '$help' in content_lower:
                return 'help'
        
        return None
    
    @staticmethod
    async def handle_backend_command(request: Request, chat_request, command: str) -> Response:
        """å¤„ç†åå°å‘½ä»¤å¹¶è¿”å›å“åº”"""
        from src.workflow.tools.scenario_table_tools import scenario_manager
        from config.manager import settings
        
        request_id = str(uuid.uuid4())
        
        try:
            if command == "reset":
                # è°ƒç”¨check_last_ai_response_index_workflowæ¥æ™ºèƒ½åˆ¤æ–­åˆé€‚çš„index
                from src.workflow.graph.check_last_ai_response_index_workflow import create_check_index_workflow
                
                # è·å–åŸå§‹æ¶ˆæ¯æ•°æ®
                original_messages = [msg.model_dump() for msg in chat_request.messages]
                
                # åˆ›å»ºå¹¶è¿è¡Œå·¥ä½œæµ
                workflow = create_check_index_workflow()
                recommended_index = await workflow.run(original_messages)
                
                if recommended_index > 0:
                    # åŠ¨æ€æ›´æ–°å†…å­˜ä¸­çš„é…ç½®å€¼
                    settings.langgraph.last_ai_messages_index = recommended_index
                    
                    message = f"""âœ… å†…å­˜ä¸­çš„ last_ai_messages_index å·²æˆåŠŸæ›´æ–°ä¸º: {recommended_index}

ğŸ”§ é€‚é…å®Œæˆï¼ç³»ç»Ÿå·²æ ¹æ®å½“å‰å¯¹è¯å†å²æ™ºèƒ½åˆ¤æ–­å¹¶è®¾ç½®äº†åˆé€‚çš„AIæ¶ˆæ¯ç´¢å¼•ã€‚

âš ï¸  é‡è¦æé†’ï¼š
â€¢ æ­¤ä¿®æ”¹ä»…åœ¨å½“å‰ç¨‹åºè¿è¡ŒæœŸé—´æœ‰æ•ˆ
â€¢ ç¨‹åºé‡å¯åå°†æ¢å¤ä¸ºé…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
â€¢ å¦‚æœæ‚¨ä¸ç»å¸¸æ›´æ¢è§’è‰²é¢„è®¾ï¼Œå»ºè®®æ‰‹åŠ¨å°†é…ç½®æ–‡ä»¶ config/config.yaml ä¸­çš„ langgraph.last_ai_messages_index ä¿®æ”¹ä¸º: {recommended_index}

ğŸ“– è¯´æ˜ï¼šlast_ai_messages_index={recommended_index} è¡¨ç¤ºä½¿ç”¨å€’æ•°ç¬¬{recommended_index}æ¡AIæ¶ˆæ¯ä½œä¸ºçœŸå®çš„è§’è‰²æ‰®æ¼”å›å¤ã€‚"""
                else:
                    message = "âŒ è‡ªåŠ¨åˆ¤æ–­å¤±è´¥ï¼Œæœªèƒ½æ‰¾åˆ°åˆé€‚çš„AIæ¶ˆæ¯ç´¢å¼•ã€‚è¯·æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„assistantæ¶ˆæ¯ã€‚"
                    
            elif command == "rm":
                # æ¸…ç©ºè¡¨æ ¼æ•°æ®
                scenario_manager.init(settings.scenario.file_path)
                table_reset = scenario_manager.reset()
                
                # æ¸…ç©ºscenariosç›®å½•
                directory_clear = DirectoryUtils.clear_scenarios_directory()
                
                if table_reset and directory_clear:
                    message = "Memory tables and scenarios directory have been reset successfully."
                else:
                    message = "Reset operation completed with some warnings."
                    
            elif command == "show":
                # æ˜¾ç¤ºè¡¨æ ¼æ•°æ®
                scenario_manager.init(settings.scenario.file_path)
                tables_content = scenario_manager.get_all_pretty_tables(description=True, operation_guide=True)
                message = f"Current Memory Tables:\\n\\n{tables_content}"
                    
            elif command == "workflow_switch_fast":
                # åˆ‡æ¢åˆ°å¿«é€Ÿå·¥ä½œæµæ¨¡å¼
                current_mode = settings.agent.workflow_mode
                settings.agent.workflow_mode = "fast"
                message = f"âœ… å·²ç»å°†æƒ…æ™¯å·¥ä½œæµä» {current_mode} è½¬æ¢ä¸º fast\n\nğŸš€ å¿«é€Ÿæ¨¡å¼ç‰¹ç‚¹ï¼š\nâ€¢ ä½¿ç”¨å¿«é€Ÿç»æµçš„å·¥ä½œæµ\nâ€¢ 2æ¬¡LLMè°ƒç”¨å®ç°è®°å¿†æœç´¢å’Œæƒ…æ™¯æ›´æ–°\nâ€¢ å“åº”é€Ÿåº¦æ›´å¿«ï¼Œæˆæœ¬æ›´ä½\n\nâš ï¸  é‡è¦æé†’ï¼š\nâ€¢ æ­¤ä¿®æ”¹ä»…åœ¨å½“å‰ç¨‹åºè¿è¡ŒæœŸé—´æœ‰æ•ˆ\nâ€¢ ç¨‹åºé‡å¯åå°†æ¢å¤ä¸ºé…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼"
                
            elif command == "workflow_switch_drp":
                # åˆ‡æ¢åˆ°æ·±åº¦è§’è‰²æ‰®æ¼”å·¥ä½œæµæ¨¡å¼
                current_mode = settings.agent.workflow_mode
                settings.agent.workflow_mode = "drp"
                message = f"âœ… å·²ç»å°†æƒ…æ™¯å·¥ä½œæµä» {current_mode} è½¬æ¢ä¸º drp\n\nğŸ§  æ·±åº¦è§’è‰²æ‰®æ¼”æ¨¡å¼ç‰¹ç‚¹ï¼š\nâ€¢ ä½¿ç”¨çµæ´»ä½†æ˜‚è´µçš„ReActå·¥ä½œæµ\nâ€¢ å¤šè½®æ¨ç†å’Œå·¥å…·è°ƒç”¨\nâ€¢ è§’è‰²æ‰®æ¼”æ·±åº¦æ›´é«˜ï¼Œä½†æˆæœ¬è¾ƒé«˜\n\nâš ï¸  é‡è¦æé†’ï¼š\nâ€¢ æ­¤ä¿®æ”¹ä»…åœ¨å½“å‰ç¨‹åºè¿è¡ŒæœŸé—´æœ‰æ•ˆ\nâ€¢ ç¨‹åºé‡å¯åå°†æ¢å¤ä¸ºé…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼"
                
            elif command == "help":
                # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
                message = """ğŸ“š DeepRolePlay å‘½ä»¤å¸®åŠ©

å½“å‰ç‰ˆæœ¬æ”¯æŒç›´æ¥åœ¨å¯¹è¯ä¸­è¾“å…¥å‘½ä»¤ï¼Œæ— éœ€è¿›å…¥ç‰¹æ®Šæ¨¡å¼ã€‚

ğŸ”§ å¯ç”¨å‘½ä»¤ï¼š
â€¢ $help - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
â€¢ $fast - åˆ‡æ¢åˆ°å¿«é€Ÿå·¥ä½œæµæ¨¡å¼ï¼ˆå¿«é€Ÿç»æµï¼‰
â€¢ $drp - åˆ‡æ¢åˆ°æ·±åº¦è§’è‰²æ‰®æ¼”å·¥ä½œæµæ¨¡å¼ï¼ˆçµæ´»ä½†æ˜‚è´µï¼‰
â€¢ $reset - æ™ºèƒ½é€‚é…AIæ¶ˆæ¯ç´¢å¼•ï¼Œè‡ªåŠ¨åˆ¤æ–­çœŸå®çš„è§’è‰²æ‰®æ¼”å›å¤
â€¢ $rm - æ¸…ç©ºæ‰€æœ‰è¡¨æ ¼æ•°æ®å’Œscenarioæ–‡ä»¶  
â€¢ $show - æ˜¾ç¤ºå½“å‰æ‰€æœ‰è¡¨æ ¼æ•°æ®"""
                    
            else:
                message = "Unknown command. Available commands: $help, $fast, $drp, $reset, $rm, $show"
            
            # åˆ›å»ºå“åº”
            response_data = ResponseBuilder.create_special_response(
                "backend_command", request_id, chat_request.model, chat_request.stream
            )
            
            # æ›´æ–°å“åº”æ¶ˆæ¯
            if "choices" in response_data and len(response_data["choices"]) > 0:
                choice = response_data["choices"][0]
                if "message" in choice:
                    choice["message"]["content"] = message
                elif "delta" in choice:
                    choice["delta"]["content"] = message
            
            if chat_request.stream:
                return await StreamingHandler.create_simple_streaming_response(
                    request, response_data, request_id
                )
            else:
                return JSONResponse(content=response_data)
                
        except Exception as e:
            print(f"Backend command error: {e}")
            error_response = ResponseBuilder.create_error_response(
                error_message=f"Backend command failed: {str(e)}",
                error_type="backend_error"
            )
            return JSONResponse(content=error_response, status_code=500)


class DirectoryUtils:
    """ç›®å½•æ“ä½œå·¥å…·ç±»"""
    
    @staticmethod
    def clear_scenarios_directory() -> bool:
        """æ¸…ç©ºscenariosç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        try:
            scenarios_path = os.path.join(os.getcwd(), "scenarios")
            if os.path.exists(scenarios_path):
                files = glob.glob(os.path.join(scenarios_path, "*"))
                for file_path in files:
                    if os.path.isfile(file_path):
                        os.remove(file_path)
            return True
        except Exception as e:
            print(f"Failed to clear scenarios directory: {e}")
            return False


class SpecialRequestHandler:
    """ç»Ÿä¸€çš„ç‰¹æ®Šè¯·æ±‚å¤„ç†å™¨"""
    
    @staticmethod
    async def handle_special_request(
        request: Request,
        chat_request,
        request_type: str
    ):
        """ç»Ÿä¸€å¤„ç†ç‰¹æ®Šè¯·æ±‚ï¼ˆè°ƒè¯•æ¨¡å¼ã€æ–°å¯¹è¯ç­‰ï¼‰"""
        request_id = str(uuid.uuid4())
        
        # ç‰¹æ®Šæ“ä½œå¤„ç†
        # ç›®å‰åªæ”¯æŒdebugæ¨¡å¼ï¼Œnew_conversationå·²ç§»è‡³DRPåå°æ¨¡å¼
        
        # åˆ›å»ºå“åº”
        response_data = ResponseBuilder.create_special_response(
            request_type, request_id, chat_request.model, chat_request.stream
        )
        
        # å‡†å¤‡å¤´éƒ¨ä¿¡æ¯å’Œæ—¥å¿—æ•°æ®
        extra_headers = {f"X-{request_type.replace('_', '-').title()}": "true"}
        log_data = {
            "request_body": {
                f"{request_type}_mode": True,
                "model": chat_request.model
            },
            "response_body": {
                "message": f"{request_type} message",
                f"{request_type}": True,
                "stream": chat_request.stream
            }
        }
        
        if chat_request.stream:
            return await StreamingHandler.create_simple_streaming_response(
                request=request,
                response_data=response_data,
                request_id=request_id,
                extra_headers=extra_headers,
                log_data=log_data
            )
        else:
            response = JSONResponse(
                content=response_data,
                status_code=200,
                headers={**extra_headers, "X-Request-ID": request_id}
            )
            
            # è®°å½•æ—¥å¿—
            await LoggingUtils.log_response(
                request=request,
                response=response,
                request_body=log_data["request_body"],
                response_body=log_data["response_body"],
                duration=0.001,
                request_id=request_id
            )
            
            return response