import json
import time
import uuid
import httpx
import asyncio
from pathlib import Path
from fastapi import APIRouter, Request, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional, Union

from config.manager import settings
from .proxy_utils import (
    ResponseBuilder, 
    AuthUtils, 
    StreamingHandler, 
    WorkflowHelper, 
    LoggingUtils, 
    DirectoryUtils,
    SpecialRequestHandler,
    BackendCommandHandler
)


class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    stream: Optional[bool] = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    stop: Optional[List[str]] = None
    
    class Config:
        extra = "allow"  # å…è®¸é¢å¤–å­—æ®µï¼Œå¦‚ thinking ç­‰æ‰©å±•å‚æ•°


router = APIRouter()


def _parse_upstream_error(response: httpx.Response) -> Dict[str, Any]:
    """Parse error response from the upstream service, maintaining the original format."""
    try:
        error_data = response.json()
        return error_data
    except (json.JSONDecodeError, ValueError):
        return ResponseBuilder.create_error_response(
            error_message=response.text or f"HTTP {response.status_code} Error",
            error_type="upstream_error",
            error_code=str(response.status_code)
        )




class ProxyService:
    def __init__(self):
        self.target_url = f"{settings.proxy.target_url.rstrip('/')}/chat/completions"
        self.models_url = settings.proxy.get_models_url()
        self.timeout = settings.proxy.timeout
        self.message_cache = []  # ç”¨äºæƒ…æ™¯æ¸…ç†ç­–ç•¥çš„æ¶ˆæ¯ç¼“å­˜
    
    
    async def forward_non_streaming_request(
        self,
        request: Request,
        chat_request: ChatCompletionRequest
    ):
        """ä½¿ç”¨ScenarioManagerçš„éæµå¼è¯·æ±‚å¤„ç†"""
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        # å¤„ç†æƒ…æ™¯æ¸…ç†ç­–ç•¥
        cleared, self.message_cache = WorkflowHelper.handle_scenario_clear_strategy(
            chat_request.messages, self.message_cache
        )
        
        from src.scenario.manager import scenario_manager
        from utils.format_converter import convert_final_response
        
        try:
            workflow_input = WorkflowHelper.prepare_workflow_input(
                request, chat_request, request_id, current_scenario=""
            )
            workflow_input["stream"] = False
            
            # 1. å…ˆæ›´æ–°åœºæ™¯
            await scenario_manager.update_scenario(workflow_input)
            
            # 2. å›¾ç‰‡ç”Ÿæˆå¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            image_generation_task = None
            if settings.comfyui.enabled:
                print(f"ğŸ–¼ï¸ Starting image generation workflow for non-streaming...", flush=True)
                from src.workflow.graph.image_generation_workflow import create_image_generation_workflow
                
                # åˆ›å»ºå¹¶å¯åŠ¨å›¾ç‰‡ç”Ÿæˆå·¥ä½œæµï¼ˆå¼‚æ­¥åå°ä»»åŠ¡ï¼‰
                workflow = create_image_generation_workflow()
                image_generation_task = asyncio.create_task(workflow.ainvoke({}))
                print(f"ğŸ–¼ï¸ Image generation task created for non-streaming", flush=True)
            
            # 3. è°ƒç”¨ç‹¬ç«‹çš„éæµå¼LLMè½¬å‘å‡½æ•°
            from src.workflow.graph.forward_workflow import forward_to_llm_non_streaming
            
            llm_response = await forward_to_llm_non_streaming(
                original_messages=workflow_input["original_messages"],
                api_key=workflow_input["api_key"],
                chat_request=chat_request
            )
            
            # 4. ç­‰å¾…å›¾ç‰‡ç”Ÿæˆå®Œæˆå¹¶åˆå¹¶å“åº”å†…å®¹
            response_content = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)
            
            if image_generation_task:
                print(f"ğŸ–¼ï¸ Waiting for image generation to complete for non-streaming...", flush=True)
                try:
                    # ç­‰å¾…å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆè®¾ç½®è¶…æ—¶ï¼‰
                    result = await asyncio.wait_for(image_generation_task, timeout=120)
                    image_paths = result.get('generated_image_paths', [])
                    print(f"ğŸ–¼ï¸ Image generation completed for non-streaming. Paths: {image_paths}", flush=True)
                    
                    # å¦‚æœæœ‰ç”Ÿæˆçš„å›¾ç‰‡ï¼Œæ·»åŠ åˆ°å“åº”å†…å®¹
                    if image_paths:
                        print(f"ğŸ–¼ï¸ Processing {len(image_paths)} generated images for non-streaming...", flush=True)
                        image_content_parts = []
                        
                        for i, image_path in enumerate(image_paths):
                            if image_path and not image_path.startswith("é”™è¯¯") and Path(image_path).exists():
                                try:
                                    print(f"ğŸ–¼ï¸ Processing image {i+1}: {image_path} for non-streaming", flush=True)
                                    # ä½¿ç”¨å›¾ç‰‡ä¼˜åŒ–å™¨å¤„ç†å›¾ç‰‡
                                    from utils.image_optimizer import optimize_and_format_image
                                    
                                    # ç”Ÿæˆä¼˜åŒ–çš„Markdownæ ¼å¼å›¾ç‰‡
                                    image_markdown = optimize_and_format_image(
                                        image_path=image_path,
                                        alt_text=f"Generated Image {i+1}",
                                        collapsible=False
                                    )
                                    
                                    if image_markdown:
                                        image_content_parts.append(image_markdown)
                                        print(f"ğŸ–¼ï¸ Successfully processed image {i+1} for non-streaming", flush=True)
                                    
                                except Exception as img_error:
                                    print(f"ğŸ–¼ï¸ Error reading image {i+1} for non-streaming: {str(img_error)}", flush=True)
                                    image_content_parts.append(f"[å›¾ç‰‡è¯»å–å¤±è´¥: {str(img_error)}]")
                            elif image_path and image_path.startswith("é”™è¯¯"):
                                # æ˜¾ç¤ºç”Ÿæˆå¤±è´¥çš„å›¾ç‰‡é”™è¯¯ä¿¡æ¯
                                print(f"ğŸ–¼ï¸ Image generation error for non-streaming: {image_path}", flush=True)
                                image_content_parts.append(f"[{image_path}]")
                        
                        # å°†å›¾ç‰‡å†…å®¹æ·»åŠ åˆ°å“åº”ä¸­
                        if image_content_parts:
                            response_content += "\n\n" + "\n\n".join(image_content_parts)
                    else:
                        print(f"ğŸ–¼ï¸ No valid image paths found for non-streaming", flush=True)
                        
                except asyncio.TimeoutError:
                    print(f"ğŸ–¼ï¸ Image generation timeout for non-streaming!", flush=True)
                    response_content += "\n\n[å›¾ç‰‡ç”Ÿæˆè¶…æ—¶]"
                except Exception as e:
                    print(f"ğŸ–¼ï¸ Image generation error for non-streaming: {str(e)}", flush=True)
                    response_content += f"\n\n[å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {str(e)}]"
            else:
                print(f"ğŸ–¼ï¸ No image generation task for non-streaming (comfyui.enabled: {settings.comfyui.enabled})", flush=True)
            
            # 5. è½¬æ¢ä¸ºOpenAIæ ¼å¼å“åº”ï¼ˆä½¿ç”¨æ›´æ–°åçš„å†…å®¹ï¼‰
            response_data = convert_final_response(response_content, chat_request.model, stream=False)
            
            duration = time.time() - start_time
            
            response = JSONResponse(content=response_data)
            
            return response
            
        except Exception as e:
            duration = time.time() - start_time
            error_data = ResponseBuilder.create_error_response(
                error_message=str(e),
                error_type="workflow_error",
                error_code="WORKFLOW_ERROR"
            )
            
            response = JSONResponse(content=error_data, status_code=500)
            
            return response
    
    

    
    def forward_streaming_request(
        self,
        request: Request,
        chat_request: ChatCompletionRequest
    ):
        """ä½¿ç”¨ScenarioManagerçš„æµå¼è¯·æ±‚å¤„ç†"""
        request_id = str(uuid.uuid4())
        
        # å¤„ç†æƒ…æ™¯æ¸…ç†ç­–ç•¥
        cleared, self.message_cache = WorkflowHelper.handle_scenario_clear_strategy(
            chat_request.messages, self.message_cache
        )
        
        from src.scenario.manager import scenario_manager
        from utils.format_converter import convert_chunk_to_sse, convert_workflow_event_to_sse, convert_chunk_to_sse_manual, create_done_message, create_reasoning_start_chunk, create_reasoning_end_chunk
        
        async def stream_generator():
            """ç”Ÿæˆå™¨ï¼Œç”¨äºå¤„ç†å·¥ä½œæµå¹¶æµå¼ä¼ è¾“LLMå“åº”"""
            try:
                # 1. å‡†å¤‡å·¥ä½œæµè¾“å…¥
                workflow_input = WorkflowHelper.prepare_workflow_input(
                    request, chat_request, request_id, current_scenario=""
                )
                workflow_input["stream"] = True
                
                # 2. æ™ºèƒ½ä½“æ¨ç†å¼€å§‹æ ‡è®°
                if settings.langgraph.stream_workflow_to_frontend:
                    agent_start_chunk = create_reasoning_start_chunk(chat_request.model, request_id)
                    yield agent_start_chunk
                
                # 3. ä½¿ç”¨ScenarioManagerçš„æµå¼æ–¹æ³•
                async for event in scenario_manager.update_scenario_streaming(workflow_input):
                    # ä½¿ç”¨ç»¼åˆçš„å·¥ä½œæµäº‹ä»¶è½¬æ¢å‡½æ•°ï¼Œæ”¯æŒå¤šç§äº‹ä»¶ç±»å‹
                    if settings.langgraph.stream_workflow_to_frontend:
                        sse_chunk = convert_workflow_event_to_sse(event, chat_request.model, request_id)
                        if sse_chunk:
                            yield sse_chunk
                
                # 4. æ™ºèƒ½ä½“æ¨ç†ç»“æŸæ ‡è®°
                if settings.langgraph.stream_workflow_to_frontend:
                    agent_end_chunk = create_reasoning_end_chunk(chat_request.model, request_id)
                    yield agent_end_chunk
                
                # 5. å›¾ç‰‡ç”Ÿæˆå¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                image_generation_task = None
                if settings.comfyui.enabled:
                    print(f"ğŸ–¼ï¸ Starting image generation workflow...", flush=True)
                    from src.workflow.graph.image_generation_workflow import create_image_generation_workflow
                    
                    # åˆ›å»ºå¹¶å¯åŠ¨å›¾ç‰‡ç”Ÿæˆå·¥ä½œæµï¼ˆå¼‚æ­¥åå°ä»»åŠ¡ï¼‰
                    workflow = create_image_generation_workflow()
                    image_generation_task = asyncio.create_task(workflow.ainvoke({}))
                    print(f"ğŸ–¼ï¸ Image generation task created", flush=True)
                
                # 6. è°ƒç”¨ç‹¬ç«‹çš„LLMè½¬å‘å‡½æ•°è¿›è¡Œæµå¼è¾“å‡º
                from src.workflow.graph.forward_workflow import forward_to_llm_streaming
                
                async for chunk in forward_to_llm_streaming(
                    original_messages=workflow_input["original_messages"],
                    api_key=workflow_input["api_key"], 
                    chat_request=chat_request
                ):
                    sse_chunk = convert_chunk_to_sse(chunk, chat_request.model, request_id)
                    if sse_chunk:
                        yield sse_chunk
                
                # # æµ‹è¯•ç”¨ï¼šæ„é€ æ¨¡æ‹Ÿçš„ image_generation_task
                # async def mock_image_generation():
                #     """æ¨¡æ‹Ÿå›¾ç‰‡ç”Ÿæˆä»»åŠ¡ï¼Œè¿”å›æµ‹è¯•å›¾ç‰‡è·¯å¾„"""
                #     await asyncio.sleep(0.1)  # æ¨¡æ‹ŸçŸ­æš‚å»¶è¿Ÿ
                #     return {
                #         'generated_image_paths': ['logs/imgs/ComfyUI_00618_.png']
                #     }

                # # åˆ›å»ºæ¨¡æ‹Ÿä»»åŠ¡
                # image_generation_task = asyncio.create_task(mock_image_generation())
                # print(f"ğŸ–¼ï¸ Mock image generation task created for testing", flush=True)
                
                # 6.5 æ£€æŸ¥å›¾ç‰‡ç”Ÿæˆæ˜¯å¦å®Œæˆå¹¶å‘é€å›¾ç‰‡
                if image_generation_task:
                    print(f"ğŸ–¼ï¸ Checking image generation task...", flush=True)
                    try:
                        # ç­‰å¾…å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆè®¾ç½®è¶…æ—¶ï¼‰
                        print(f"ğŸ–¼ï¸ Waiting for image generation to complete (timeout=120s)...", flush=True)
                        result = await asyncio.wait_for(image_generation_task, timeout=120)
                        image_paths = result.get('generated_image_paths', [])
                        print(f"ğŸ–¼ï¸ Image generation completed. Paths: {image_paths}", flush=True)
                        
                        # å¦‚æœæœ‰ç”Ÿæˆçš„å›¾ç‰‡ï¼Œå‘é€åˆ°å‰ç«¯
                        if image_paths:
                            print(f"ğŸ–¼ï¸ Processing {len(image_paths)} generated images...", flush=True)
                            for i, image_path in enumerate(image_paths):
                                if image_path and not image_path.startswith("é”™è¯¯") and Path(image_path).exists():
                                    try:
                                        print(f"ğŸ–¼ï¸ Processing image {i+1}: {image_path}", flush=True)
                                        # ä½¿ç”¨å›¾ç‰‡ä¼˜åŒ–å™¨å¤„ç†å›¾ç‰‡
                                        from utils.image_optimizer import optimize_and_format_image
                                        
                                        # ç”Ÿæˆä¼˜åŒ–çš„Markdownæ ¼å¼å›¾ç‰‡ï¼ˆç›´æ¥è°ƒæ•´åˆ°512pxå°ºå¯¸ï¼‰
                                        image_markdown = optimize_and_format_image(
                                            image_path=image_path,
                                            alt_text=f"Generated Image {i+1}",
                                            collapsible=False
                                        )
                                        
                                        # åˆ›å»ºåŒ…å«ä¼˜åŒ–å›¾ç‰‡çš„æ¶ˆæ¯å—ï¼ˆå•ä¸ªæ¶ˆæ¯å‘é€ï¼‰
                                        image_chunk = convert_chunk_to_sse_manual(f"\n{image_markdown}\n", chat_request.model, request_id)
                                        print(f"ğŸ–¼ï¸ Sending optimized image {i+1} via SSE (Markdown format)...", flush=True)
                                        yield image_chunk
                                        
                                    except Exception as img_error:
                                        print(f"ğŸ–¼ï¸ Error reading image {i+1}: {str(img_error)}", flush=True)
                                        error_chunk = convert_chunk_to_sse_manual(f"\n[å›¾ç‰‡è¯»å–å¤±è´¥: {str(img_error)}]\n", chat_request.model, request_id)
                                        yield error_chunk
                                elif image_path and image_path.startswith("é”™è¯¯"):
                                    # æ˜¾ç¤ºç”Ÿæˆå¤±è´¥çš„å›¾ç‰‡é”™è¯¯ä¿¡æ¯
                                    print(f"ğŸ–¼ï¸ Image generation error: {image_path}", flush=True)
                                    error_chunk = convert_chunk_to_sse_manual(f"\n[{image_path}]\n", chat_request.model, request_id)
                                    yield error_chunk
                        else:
                            print(f"ğŸ–¼ï¸ No valid image paths found", flush=True)
                                    
                    except asyncio.TimeoutError:
                        # å¦‚æœè¶…æ—¶ï¼Œå‘é€é”™è¯¯ä¿¡æ¯
                        print(f"ğŸ–¼ï¸ Image generation timeout!", flush=True)
                        error_chunk = convert_chunk_to_sse_manual("\n[å›¾ç‰‡ç”Ÿæˆè¶…æ—¶]\n", chat_request.model, request_id)
                        yield error_chunk
                    except Exception as e:
                        # å…¶ä»–é”™è¯¯
                        print(f"ğŸ–¼ï¸ Image generation error: {str(e)}", flush=True)
                        error_chunk = convert_chunk_to_sse_manual(f"\n[å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {str(e)}]\n", chat_request.model, request_id)
                        yield error_chunk
                else:
                    print(f"ğŸ–¼ï¸ No image generation task (comfyui.enabled: {settings.comfyui.enabled})", flush=True)
                
                # 7. å‘é€ç»“æŸä¿¡å·
                yield create_done_message()

            except Exception as e:
                import traceback
                print(f"Error during streaming: {traceback.format_exc()}")
                error_data = ResponseBuilder.create_error_response(
                    error_message=str(e),
                    error_type="workflow_error",
                    error_code="WORKFLOW_STREAM_ERROR"
                )
                error_chunk = f"data: {json.dumps(error_data)}\n\n"
                yield error_chunk
                yield create_done_message()

        return StreamingResponse(stream_generator(), media_type="text/event-stream")
    
    async def forward_models_request(self, request: Request):
        """Forward a models query request to the target LLM service."""
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(
                    self.models_url,
                    headers=AuthUtils.get_request_headers(request)
                )
                
                duration = time.time() - start_time
                
                if response.status_code >= 400:
                    error_data = _parse_upstream_error(response)
                    json_response = JSONResponse(content=error_data, status_code=response.status_code)
                else:
                    response_data = response.json()
                    json_response = JSONResponse(content=response_data)
                    error_data = response_data
                
                await LoggingUtils.log_response(
                    request=request,
                    response=json_response,
                    request_body={},
                    response_body=error_data,
                    duration=duration,
                    request_id=request_id
                )
                
                return json_response
                    
        except httpx.RequestError as e:
            duration = time.time() - start_time
            error_data = {"error": f"Request error: {str(e)}"}
            
            await LoggingUtils.log_response(
                request=request,
                response=None,
                request_body={},
                response_body=error_data,
                duration=duration,
                request_id=request_id
            )
            
            raise HTTPException(
                status_code=502,
                detail=f"Could not connect to the upstream service: {str(e)}"
            )


proxy_service = ProxyService()


@router.post("/v1/chat/completions")
async def chat_completions(request: Request, chat_request: ChatCompletionRequest):
    """OpenAI-compatible chat completion endpoint."""
    
    try:
        # ä¿å­˜å®Œæ•´çš„è¯·æ±‚å‚æ•°ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
        request_id = str(uuid.uuid4())
        await LoggingUtils.save_full_messages(chat_request, request_id)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åå°å‘½ä»¤éœ€è¦å¤„ç†
        command = BackendCommandHandler.parse_command_from_messages(chat_request.messages, 10)
        if command:
            print(f"ğŸ” Parsed command: {command}")
            response = await BackendCommandHandler.handle_backend_command(request, chat_request, command)
        # è°ƒè¯•æ¨¡å¼
        elif settings.proxy.debug_mode:
            response = await SpecialRequestHandler.handle_special_request(request, chat_request, "debug")
        # æ­£å¸¸æµå¼/éæµå¼è¯·æ±‚å¤„ç†
        elif chat_request.stream:
            response = proxy_service.forward_streaming_request(request, chat_request)
        else:
            response = await proxy_service.forward_non_streaming_request(request, chat_request)
        
        return response
        
    except Exception as e:
        # å¦‚æœä»»ä½•æ­¥éª¤å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªæ ‡å‡†çš„é”™è¯¯å“åº”
        print(f"ğŸ’€ CRITICAL ERROR in chat_completions: {e}")
        error_data = ResponseBuilder.create_error_response(
            error_message=f"An unexpected error occurred: {str(e)}",
            error_type="internal_server_error",
            error_code="UNEXPECTED_ERROR"
        )
        return JSONResponse(content=error_data, status_code=500)


@router.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "version": "1.0.0"}


@router.get("/v1/models")
async def list_models(request: Request):
    """OpenAI-compatible model listing endpoint."""
    return await proxy_service.forward_models_request(request)