#!/usr/bin/env python3
"""
ä¸“é—¨æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½ - è¿™æ˜¯æœ€é‡è¦çš„æµ‹è¯•
æµ‹è¯•å®Œæ•´çš„å·¥ä½œæµæµå¼è¾“å‡ºï¼ŒéªŒè¯OpenAIæ ¼å¼è½¬æ¢
"""

import asyncio
import json
import sys
import os
import time
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.abspath(os.path.dirname(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.workflow.graph.scenario_workflow import create_scenario_workflow
from utils.format_converter import convert_to_openai_sse, create_done_message, extract_content_from_event

class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    PURPLE = '\033[95m'
    CYAN = '\033[96m'
    BOLD = '\033[1m'
    END = '\033[0m'

def print_header(title: str):
    print(f"\n{Colors.CYAN}{Colors.BOLD}{'='*60}{Colors.END}")
    print(f"{Colors.CYAN}{Colors.BOLD}{title:^60}{Colors.END}")
    print(f"{Colors.CYAN}{Colors.BOLD}{'='*60}{Colors.END}")

async def test_workflow_streaming():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµçš„æµå¼è¾“å‡º"""
    print_header("æµ‹è¯•å·¥ä½œæµæµå¼è¾“å‡ºï¼ˆæœ€é‡è¦çš„åŠŸèƒ½ï¼‰")
    
    try:
        # åˆ›å»ºå·¥ä½œæµ
        workflow = create_scenario_workflow()
        print(f"{Colors.GREEN}âœ“ å·¥ä½œæµåˆ›å»ºæˆåŠŸ{Colors.END}")
        
        # å‡†å¤‡æµ‹è¯•è¾“å…¥
        test_input = {
            "request_id": "streaming-test-123",
            "original_messages": [
                {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹"},
                {"role": "assistant", "content": "Pythonæ˜¯ä¸€é—¨å¾ˆå¥½çš„ç¼–ç¨‹è¯­è¨€ï¼Œé€‚åˆåˆå­¦è€…ã€‚"},
                {"role": "user", "content": "èƒ½æ¨èä¸€äº›å­¦ä¹ èµ„æºå—ï¼Ÿ"}
            ],
            "messages": [
                {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹"},
                {"role": "assistant", "content": "Pythonæ˜¯ä¸€é—¨å¾ˆå¥½çš„ç¼–ç¨‹è¯­è¨€ï¼Œé€‚åˆåˆå­¦è€…ã€‚"},
                {"role": "user", "content": "èƒ½æ¨èä¸€äº›å­¦ä¹ èµ„æºå—ï¼Ÿ"}
            ],
            "current_scenario": "",
            "api_key": "sk-5b155b212651493b942e7dca7dfb4751",
            "model": "deepseek-chat",
            "stream": True
        }
        
        print(f"{Colors.BLUE}â„¹ å¼€å§‹æµå¼æ‰§è¡Œå·¥ä½œæµ...{Colors.END}")
        print(f"{Colors.BLUE}â„¹ é¢„æœŸè¾“å‡ºé¡ºåº: è®°å¿†é—ªå› â†’ æƒ…æ™¯æ›´æ–° â†’ LLMæœ€ç»ˆå›å¤{Colors.END}")
        
        # æ”¶é›†æµå¼è¾“å‡º
        stream_chunks = []
        openai_chunks = []
        node_outputs = {"memory": [], "scenario": [], "llm": []}
        
        start_time = time.time()
        chunk_count = 0
        
        # ä½¿ç”¨astreamè·å–æµå¼æ¶ˆæ¯
        async for msg, metadata in workflow.astream(
            test_input,
            stream_mode="messages"
        ):
            chunk_count += 1
            current_node = metadata.get("langgraph_node", "unknown") if metadata else "unknown"
            
            if hasattr(msg, 'content') and msg.content:
                content = msg.content
                stream_chunks.append(content)
                
                # æŒ‰èŠ‚ç‚¹åˆ†ç±»æ”¶é›†è¾“å‡º
                if "memory" in current_node.lower():
                    node_outputs["memory"].append(content)
                elif "scenario" in current_node.lower():
                    node_outputs["scenario"].append(content)
                elif "llm" in current_node.lower() or "forwarding" in current_node.lower():
                    node_outputs["llm"].append(content)
                
                # è½¬æ¢ä¸ºOpenAIæ ¼å¼
                try:
                    sse_chunk = convert_to_openai_sse(msg, metadata, test_input["model"])
                    openai_chunks.append(sse_chunk)
                    
                    # éªŒè¯SSEæ ¼å¼
                    if not sse_chunk.startswith("data: "):
                        print(f"{Colors.RED}âŒ SSEæ ¼å¼é”™è¯¯: ä¸ä»¥'data: 'å¼€å¤´{Colors.END}")
                    else:
                        # è§£æJSONéªŒè¯
                        json_str = sse_chunk[6:-2]  # ç§»é™¤"data: "å’Œ"\n\n"
                        json.loads(json_str)  # éªŒè¯JSONæ ¼å¼
                    
                except Exception as e:
                    print(f"{Colors.RED}âŒ OpenAIæ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}{Colors.END}")
                
                # å®æ—¶æ˜¾ç¤ºæµå¼è¾“å‡º
                if len(content) > 50:
                    display_content = content[:50] + "..."
                else:
                    display_content = content
                    
                print(f"{Colors.GREEN}ğŸ“¤ [{current_node}] {display_content}{Colors.END}")
            
            elif isinstance(msg, dict):
                # å¤„ç†å­—å…¸æ ¼å¼æ¶ˆæ¯
                content = extract_content_from_event(msg)
                if content:
                    stream_chunks.append(content)
                    print(f"{Colors.YELLOW}ğŸ“¤ [dict] {content[:50]}...{Colors.END}")
        
        duration = time.time() - start_time
        
        # åˆ†æç»“æœ
        print_header("æµå¼è¾“å‡ºæµ‹è¯•ç»“æœåˆ†æ")
        
        print(f"{Colors.BLUE}ğŸ“Š åŸºæœ¬ç»Ÿè®¡:{Colors.END}")
        print(f"   æ€»æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
        print(f"   æµå¼å—æ•°é‡: {chunk_count}")
        print(f"   OpenAIæ ¼å¼å—: {len(openai_chunks)}")
        print(f"   æ€»å†…å®¹é•¿åº¦: {sum(len(c) for c in stream_chunks)} å­—ç¬¦")
        
        print(f"\n{Colors.BLUE}ğŸ“‹ èŠ‚ç‚¹è¾“å‡ºåˆ†å¸ƒ:{Colors.END}")
        print(f"   è®°å¿†é—ªå›èŠ‚ç‚¹: {len(node_outputs['memory'])} å—")
        print(f"   æƒ…æ™¯æ›´æ–°èŠ‚ç‚¹: {len(node_outputs['scenario'])} å—") 
        print(f"   LLMè½¬å‘èŠ‚ç‚¹: {len(node_outputs['llm'])} å—")
        
        # éªŒè¯æµå¼è¾“å‡ºå®Œæ•´æ€§
        success_checks = []
        
        # æ£€æŸ¥1: æ˜¯å¦æœ‰æµå¼è¾“å‡º
        if len(stream_chunks) > 0:
            success_checks.append(("æœ‰æµå¼è¾“å‡º", True))
            print(f"{Colors.GREEN}âœ“ æ£€æŸ¥1é€šè¿‡: äº§ç”Ÿäº† {len(stream_chunks)} ä¸ªæµå¼å—{Colors.END}")
        else:
            success_checks.append(("æœ‰æµå¼è¾“å‡º", False))
            print(f"{Colors.RED}âœ— æ£€æŸ¥1å¤±è´¥: æ²¡æœ‰æµå¼è¾“å‡º{Colors.END}")
        
        # æ£€æŸ¥2: OpenAIæ ¼å¼è½¬æ¢
        if len(openai_chunks) > 0:
            success_checks.append(("OpenAIæ ¼å¼è½¬æ¢", True))
            print(f"{Colors.GREEN}âœ“ æ£€æŸ¥2é€šè¿‡: æˆåŠŸè½¬æ¢ {len(openai_chunks)} ä¸ªOpenAIæ ¼å¼å—{Colors.END}")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªOpenAIæ ¼å¼å—æ ·ä¾‹
            if openai_chunks:
                print(f"{Colors.YELLOW}   æ ·ä¾‹SSEå—: {openai_chunks[0][:100]}...{Colors.END}")
        else:
            success_checks.append(("OpenAIæ ¼å¼è½¬æ¢", False))
            print(f"{Colors.RED}âœ— æ£€æŸ¥2å¤±è´¥: OpenAIæ ¼å¼è½¬æ¢å¤±è´¥{Colors.END}")
        
        # æ£€æŸ¥3: å¤šèŠ‚ç‚¹è¾“å‡º
        active_nodes = sum(1 for node_list in node_outputs.values() if len(node_list) > 0)
        if active_nodes >= 2:  # è‡³å°‘2ä¸ªèŠ‚ç‚¹æœ‰è¾“å‡º
            success_checks.append(("å¤šèŠ‚ç‚¹è¾“å‡º", True))
            print(f"{Colors.GREEN}âœ“ æ£€æŸ¥3é€šè¿‡: {active_nodes} ä¸ªèŠ‚ç‚¹äº§ç”Ÿäº†è¾“å‡º{Colors.END}")
        else:
            success_checks.append(("å¤šèŠ‚ç‚¹è¾“å‡º", False))
            print(f"{Colors.RED}âœ— æ£€æŸ¥3å¤±è´¥: åªæœ‰ {active_nodes} ä¸ªèŠ‚ç‚¹äº§ç”Ÿè¾“å‡º{Colors.END}")
        
        # æ£€æŸ¥4: å†…å®¹è´¨é‡
        total_content = ''.join(stream_chunks)
        if len(total_content) > 100:  # è‡³å°‘100å­—ç¬¦çš„æœ‰æ„ä¹‰å†…å®¹
            success_checks.append(("å†…å®¹è´¨é‡", True))
            print(f"{Colors.GREEN}âœ“ æ£€æŸ¥4é€šè¿‡: æ€»å†…å®¹é•¿åº¦ {len(total_content)} å­—ç¬¦{Colors.END}")
        else:
            success_checks.append(("å†…å®¹è´¨é‡", False))
            print(f"{Colors.RED}âœ— æ£€æŸ¥4å¤±è´¥: å†…å®¹è¿‡å°‘ ({len(total_content)} å­—ç¬¦){Colors.END}")
        
        # æ£€æŸ¥5: æµå¼å»¶è¿Ÿåˆç†æ€§
        avg_delay = duration / max(chunk_count, 1)
        if avg_delay < 10:  # æ¯å—å¹³å‡ä¸è¶…è¿‡10ç§’
            success_checks.append(("æµå¼å»¶è¿Ÿ", True))
            print(f"{Colors.GREEN}âœ“ æ£€æŸ¥5é€šè¿‡: å¹³å‡æ¯å—å»¶è¿Ÿ {avg_delay:.2f}ç§’{Colors.END}")
        else:
            success_checks.append(("æµå¼å»¶è¿Ÿ", False))
            print(f"{Colors.RED}âœ— æ£€æŸ¥5å¤±è´¥: å¹³å‡æ¯å—å»¶è¿Ÿè¿‡é•¿ ({avg_delay:.2f}ç§’){Colors.END}")
        
        # æ€»ä½“è¯„ä¼°
        passed_checks = sum(1 for _, passed in success_checks if passed)
        total_checks = len(success_checks)
        
        print_header("æœ€ç»ˆæµ‹è¯•ç»“æœ")
        
        if passed_checks == total_checks:
            print(f"{Colors.GREEN}{Colors.BOLD}ğŸ‰ æµå¼è¾“å‡ºæµ‹è¯•å®Œå…¨é€šè¿‡ï¼({passed_checks}/{total_checks}){Colors.END}")
            print(f"{Colors.GREEN}   å·¥ä½œæµæµå¼è¾“å‡ºåŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥ä¸ºç”¨æˆ·æä¾›å®æ—¶çš„AIæ€è€ƒè¿‡ç¨‹{Colors.END}")
            return True
        elif passed_checks >= total_checks * 0.8:  # 80%é€šè¿‡
            print(f"{Colors.YELLOW}{Colors.BOLD}âš  æµå¼è¾“å‡ºåŸºæœ¬é€šè¿‡ ({passed_checks}/{total_checks}){Colors.END}")
            print(f"{Colors.YELLOW}   ä¸»è¦åŠŸèƒ½æ­£å¸¸ï¼Œä½†æœ‰å°‘é‡é—®é¢˜éœ€è¦ä¼˜åŒ–{Colors.END}")
            return True
        else:
            print(f"{Colors.RED}{Colors.BOLD}âŒ æµå¼è¾“å‡ºæµ‹è¯•å¤±è´¥ ({passed_checks}/{total_checks}){Colors.END}")
            print(f"{Colors.RED}   æµå¼è¾“å‡ºåŠŸèƒ½å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ä¿®å¤{Colors.END}")
            return False
        
    except Exception as e:
        print(f"{Colors.RED}âŒ æµå¼è¾“å‡ºæµ‹è¯•å¼‚å¸¸: {str(e)}{Colors.END}")
        import traceback
        print(f"{Colors.RED}{traceback.format_exc()}{Colors.END}")
        return False

async def test_openai_format_conversion():
    """ä¸“é—¨æµ‹è¯•OpenAIæ ¼å¼è½¬æ¢"""
    print_header("æµ‹è¯•OpenAIæ ¼å¼è½¬æ¢åŠŸèƒ½")
    
    try:
        from langchain_core.messages import AIMessage
        from utils.format_converter import convert_to_openai_format, convert_to_openai_sse
        
        # æµ‹è¯•æ¶ˆæ¯
        test_msg = AIMessage(content="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ¶ˆæ¯")
        
        # æµ‹è¯•OpenAIæ ¼å¼è½¬æ¢
        openai_format = convert_to_openai_format(test_msg, model="deepseek-chat")
        
        # éªŒè¯æ ¼å¼
        required_fields = ["id", "object", "created", "model", "choices"]
        missing_fields = [field for field in required_fields if field not in openai_format]
        
        if not missing_fields:
            print(f"{Colors.GREEN}âœ“ OpenAIæ ¼å¼åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ{Colors.END}")
        else:
            print(f"{Colors.RED}âœ— OpenAIæ ¼å¼ç¼ºå°‘å­—æ®µ: {missing_fields}{Colors.END}")
            return False
        
        # éªŒè¯choicesç»“æ„
        choices = openai_format.get("choices", [])
        if choices and len(choices) > 0:
            choice = choices[0]
            if "delta" in choice and "content" in choice["delta"]:
                print(f"{Colors.GREEN}âœ“ choicesç»“æ„æ­£ç¡®{Colors.END}")
            else:
                print(f"{Colors.RED}âœ— choicesç»“æ„é”™è¯¯{Colors.END}")
                return False
        else:
            print(f"{Colors.RED}âœ— choiceså­—æ®µä¸ºç©º{Colors.END}")
            return False
        
        # æµ‹è¯•SSEæ ¼å¼è½¬æ¢
        sse_format = convert_to_openai_sse(test_msg, model="deepseek-chat")
        
        if sse_format.startswith("data: ") and sse_format.endswith("\n\n"):
            print(f"{Colors.GREEN}âœ“ SSEæ ¼å¼æ­£ç¡®{Colors.END}")
        else:
            print(f"{Colors.RED}âœ— SSEæ ¼å¼é”™è¯¯{Colors.END}")
            return False
        
        # éªŒè¯JSONå¯è§£ææ€§
        json_part = sse_format[6:-2]  # ç§»é™¤"data: "å’Œ"\n\n"
        try:
            parsed_json = json.loads(json_part)
            print(f"{Colors.GREEN}âœ“ SSEä¸­çš„JSONå¯æ­£ç¡®è§£æ{Colors.END}")
        except json.JSONDecodeError as e:
            print(f"{Colors.RED}âœ— SSEä¸­çš„JSONè§£æå¤±è´¥: {str(e)}{Colors.END}")
            return False
        
        print(f"{Colors.GREEN}{Colors.BOLD}ğŸ‰ OpenAIæ ¼å¼è½¬æ¢æµ‹è¯•é€šè¿‡ï¼{Colors.END}")
        return True
        
    except Exception as e:
        print(f"{Colors.RED}âŒ OpenAIæ ¼å¼è½¬æ¢æµ‹è¯•å¼‚å¸¸: {str(e)}{Colors.END}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print(f"{Colors.CYAN}ğŸš€ å¼€å§‹æµå¼è¾“å‡ºä¸“é¡¹æµ‹è¯•...{Colors.END}")
    
    # æµ‹è¯•1: OpenAIæ ¼å¼è½¬æ¢
    print("\n" + "="*60)
    format_test = await test_openai_format_conversion()
    
    # æµ‹è¯•2: å®Œæ•´å·¥ä½œæµæµå¼è¾“å‡º
    print("\n" + "="*60)
    streaming_test = await test_workflow_streaming()
    
    # æœ€ç»ˆæŠ¥å‘Š
    print_header("ä¸“é¡¹æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    
    if format_test and streaming_test:
        print(f"{Colors.GREEN}{Colors.BOLD}ğŸ† æ‰€æœ‰æµå¼è¾“å‡ºæµ‹è¯•é€šè¿‡ï¼{Colors.END}")
        print(f"{Colors.GREEN}   âœ“ OpenAIæ ¼å¼è½¬æ¢æ­£å¸¸{Colors.END}")
        print(f"{Colors.GREEN}   âœ“ å·¥ä½œæµæµå¼è¾“å‡ºæ­£å¸¸{Colors.END}")
        print(f"{Colors.GREEN}   âœ“ ç”¨æˆ·å¯ä»¥å®æ—¶çœ‹åˆ°å®Œæ•´çš„AIæ€è€ƒè¿‡ç¨‹{Colors.END}")
        print(f"{Colors.GREEN}   âœ“ é‡æ„åçš„æµå¼åŠŸèƒ½å®Œå…¨å¯ç”¨ï¼{Colors.END}")
    else:
        print(f"{Colors.RED}{Colors.BOLD}âŒ æµå¼è¾“å‡ºæµ‹è¯•å­˜åœ¨é—®é¢˜{Colors.END}")
        print(f"   OpenAIæ ¼å¼è½¬æ¢: {'âœ“' if format_test else 'âœ—'}")
        print(f"   å·¥ä½œæµæµå¼è¾“å‡º: {'âœ“' if streaming_test else 'âœ—'}")

if __name__ == "__main__":
    asyncio.run(main())